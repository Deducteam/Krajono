%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% numbers       To obtain numeric citation style instead of author/year.

\usepackage{amsmath}
\usepackage{fancyvrb}
\usepackage{bussproofs}

\newcommand{\cL}{{\cal L}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{LFMTP '16}{June 23, 2016, Porto, Portugal}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\copyrightdoi{nnnnnnn.nnnnnnn}

% Uncomment the publication rights you want to use.
%\publicationrights{transferred}
%\publicationrights{licensed}     % this is the default
%\publicationrights{author-pays}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{A lightweight implementation of an HOL system in an extension of higher order logic programming}   % 'preprint' option specified.

\title{Implementing HOL in an Higher Order Logic Programming Language}
%\subtitle{Subtitle Text, if any}

\authorinfo{Cvetan Dunchev \and Claudio Sacerdoti Coen}
           {University of Bologna}
           {cdunchev@hotmail.com/claudio.sacerdoticoen@unibo.it}
\authorinfo{Enrico Tassi}
           {INRIA Sophia-Antipolis}
           {enrico.tassi@inria.fr}

\maketitle

\begin{abstract}
We present a proof-of-concept prototype of a (constructive variant of an) HOL interactive theorem prover written in a Higher Order Logic Programming (HOLP) language, namely an extension of LambdaProlog. The prototype is meant to support the claim, that we reinforce, that HOLP is the class of languages that provides the right abstraction level and programming primitives to obtain concise implementations of theorem provers. We identify and advocate for a programming technique, that we call semi-shallow embedding, while at the same time identifying the reasons why pure LambdaProlog is not sufficient to support that technique, and it needs to be extended.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%\terms
%term1, term2

\keywords
LambdaProlog, HOL, Higher Order Logic Programming, Constraints

\section{Introduction}
What are the programming paradigm and the programming languages better suited for the implementation of interactive theorem provers? Better suited here means providing high level primitives at the good level of abstraction, relieving the programmer from the burden of (re)implementing the basic mechanisms a theorem prover relies on.

Every (interactive) theorem prover:
\begin{enumerate}
\item Has to manipulate expressions with binders up to $\alpha$-conversion, and it needs to implement substitution. Binders are ubiqutous: they occur in formulae, but also in both procedural and declarative scripts (new hypotheses are named and have an associated scope), and in proof objects as well.
\item Has to implement automatic proof search, either in the small or in the large, that requires to explore the search space via backtracking. Because the search space can be very large, the programmer needs way to control backtracking and direct the exploration.
\item Has to manipulate incomplete data structures, i.e. data structures having parts not specified yet. Moreover, invariants have to be enforced on the data structures and lazily checked when the data structures get instantiated. Examples of incomplete data occurs in formulae (omission of types to be inferred), sequents (omission of terms to be identified later, e.g. writing $X$ for a yet unknown witness of an existential statement), and proof objects (an incomplete proof has a proof object containing an hole).
\end{enumerate}

The three features discussed above also interact in complex ways. For example, renaming bound variables in a term that contains metavariables (i.e. omitted sub-terms) must record the renaming as an explicit substitution applied to the metavariable to resume the renaming when the metavariable will be istantiated. Or, backtracking must be aware of dependencies between subgoals due to sharing of metavariables in order to split the subgoals into independent clusters of goals such that once a cluster is solved it becomes useless to backtrack that solution because it won't affect the search space of the other goals.

In a series of papers from the XXs \cite{????}, Amy Felty already advocated Higher Order Logic Programming (HOLP) as the programming paradigm best suited for the three tasks above. LambdaProlog, the flagship of HOLP languages, manipulates syntax with binders, relieving the programmer from problems due to renaming and substitution; being a logic language it has backtracking for free; it allows to control backtracking via the usual Prolog's cut ("!") whose semantics and pragmatics is well known, even if it does not fit completely well the logical reading of the language; finally the language uses metavariables that range over both data and functions, and the complex implementation of LambdaProlog takes care of all interactions between binders ($\lambda$-abstraction) and metavariables, in particular addressing the problem of higher-order unification under mixed prefixes~\cite{??}.

Despite the push by Amy Felty, none of the interactive theorem provers in use is implemented in HOLP. With the exception of Mizar, implemented in Pascal for historical reasons, and the new Lean system, implemented in C++ for efficiency, all other systems (Coq, Isabelle, Agda, Matita, etc. \cite{?17thproversoftheworld??}) are implemented in either Haskell or an ML variant.

In general, functional languages provide a good compromise between performance and high-level coding, and algebraic data types can naturally encode syntax without binders. In particular, ML was born as the language for implementing the LCF theorem prover, one of the first theorem provers of the world. Nevertheless, these languages solve none of the three problems above:
\begin{enumerate}
\item with the exception of FreshML, that has also not been used for interactive theorem proving yet, functional languages do not abstract the details about binders and the user must encode bound variables via De Brujin indexes (like in Coq and Matita) or it must implement $\alpha$-conversion and substitution carefully (like in HOL Light). De Brujin indexes allow the code to be very efficient, but code that handles them is very error prone.
\item the ML and Haskell families do not provide backtracking for free. A limited form of backtracking can be implemented in ML via exceptions: an exception can be raised to backtrack to a previous state, but once a function call is over it is not possible to backtrack into it any longer. Continuations based monads to implement backtracking (plus state to handle metavariables and failure) exists~\cite{?????}, but the code is quite complicated and it has efficiency problems unless it is manually optimized (Section~X of~\cite{????}).
\item managing metavariables, their interaction with binders and higher order unification of fragments of it requires a lot of fragile and complex code (e.g. $\approx$ 3100 lines of OCaml code for Matita, compared to the $\approx$ 1500 lines for the kernel that implements $\beta$-reduction, conversion and the inference rules of the logic).
\end{enumerate}

The situation described above is bad in two respects.
\begin{enumerate}
\item All the code that deals with the three problems is essentially logic-independent, but it requires to be re-implemented when ones wants to experiment with a new logic or implement a new system. With the exception of the monad cited above, it also seems hard to encapsulate the boilerplate code into a reusable library: a real extension of the programming language is necessary.
\item The code of the systems becomes very low-level, having to deal with all kind of intricacies due to binders and metavariables. Therefore it is hard for external programmers to contribute to the code base, for example to implement new domain specific tactics. The result is that these systems often implement in user space a second programming language, exposed to the user to write tactics, that takes care of binding, metavariables, backtracking and its control. For example, LTac~\cite{???} is such a programming language for Coq, that also sports several other mini-languages to let the user customize the behaviour of the system (e.g. to declare type classes~\cite{???}, used to provide unification hints~\cite{???}). Not only the system becomes more complex because of the need to provide and interpret a new programming language on top, but its semantics is problematic: the beaviour of the system becomes the combination of pieces of code written in multiple languages and interacting in non trivial ways. Static analysis of this kind of code is out of reach.
\end{enumerate}

Adopting an HOLP language seems a promising idea. First of all the boilerplate code is pushed once and for all in the interpreter of the language, becoming decoupled from the programming logic of the theorem prover. Therefore the code of the theorem prover becomes much more clean, and very close to the inference rules it implements. Secondly, because of the decoupling, it becomes possible to experiment with alternative implementations of the HOLP language, e.g. susbtituting De Brujin levels for the indexes (like in~\cite{???}) or profiling the implementation of the language on different applications. Thirdly, and most importantly, there is no more any need for ad-hoc languages in user space: the users that want can directly contribute to the code base of the theorem proving ignoring all the gory details.

Nevertheless, as we said, no system has been implemented in LambdaProlog, despite the interest raised by the work of Felty (her XXX paper~\cite{???} has more than YYY citations). One reason is due to performance: logic languages are hard to optimize, and LambdaProlog is orders of magnitudes harder than Prolog not only because of binders and higher order unification, but also because it is higher order (one can pass predicates around) and it allows a primitive (logical implication \texttt{P => Q}) to temporarily augment the code of the program (with \texttt{P}, while proving \texttt{Q}) in a lexically scoped way, making some well known Prolog static analyses and optimizations hard. Moreover, LambdaProlog had for some time only very slow implementations, until Teyjus was born~\cite{???} after the work of Felty. Recent work by the authors also showed that the performances of Teyjus are not so good~\cite{???}, considering that Teyjus compiles the code to an extension of the Warren abstract machine instruction set, while the ELPI interpreter of the authors that attempts no compilation is able to run consistently faster than Teyjus (Section XXX of~\cite{???}).

In this paper we push seriously the idea of implementing in the ELPI variant of LambdaProlog an interactive theorem prover, inspired by HOL Light, but for a constructive variant of the logic. The aim of the experiment is to come up with implementation guidelines, that differ from the ones of Amy Felty, and to quantify what is the loss in term of performance w.r.t. systems implemented in functional languages.

We also note that our expertise acquired with the implementation of Matita was put to full already in the implementation of our ELPI interpreter. Indeed, we essentially had to split out the code of Matita that deals with the three problems above, and figure out how to integrate it in an interpreter for a logic language. In the long term, we would like to scale the HOL experiment to a new implementation of Matita/Coq (or at least its logical core, that excludes the user interface).

Interestingly enough, the methodology we identified, presented in Section~\ref{UUU}, requires significant extensions of LambdaProlog, that we partially implemented in ELPI and that we briefly describe here as well.

As a final remark, the interest of the work, that is still on-going, is not to obtain a new competitive implementation of HOL, that would require several men years and could be of limited interest. In particular, we only developed the system so far to the point where all the parts of a typical theorem prover implementation for HOL are represented to judge the feasibility and economy of the implementation.

Section~\ref{????} \ldots

\section{LambdaProlog in a nutshell}
LambdaProlog extends the core of Prolog in two directions.
\begin{enumerate}
\item it allows in the syntax $\lambda$-abstractions (written \verb+x\p+ for $\lambda x.p$), applications (written \verb+p q+ like in $\lambda$-calculus and unlike in Prolog) and metavariables both in argument and in head position (e.g. \verb+iter [X|XS] F [Y|YS] :- F X Y, iter XS F YS.+ where the metavariable \verb+F+ is to be istantiated with a binary predicate like \verb,(x\y\y=x+x),)
\item it generalizes Horn clauses to Higher Order Hereditary Harrop Formulae (HOHHF) clauses, whose grammar is
$$\begin{array}{l}
H ::= x~t_1~\ldots~t_n ~|~ H \wedge H ~|~ \forall X.H ~|~ \exists x.H ~|~ G \Rightarrow H\\
G ::= x~t_1~\ldots~t_n ~|~ X~t_1~\ldots~t_n ~|~ G \wedge G ~|~ G \vee G\\
 \hspace{0.65cm} ~|~ \forall x.G ~|~ \exists X.G ~|~ H \Rightarrow G\\
H,G \subseteq t ::= x ~|~ X ~|~ tt ~|~ x\backslash t
\end{array}$$
where $H$ ranges over HOHHF, $G$ ranges over goal formulae, $x$ are constants/universally quantified variables, $X$ are existentially quantified variables and $t$ are the higher order terms of the language. A LambdaProlog program is a list of HOHHF formulae. To run a LambdaProlog program the user submits a query $G$ that is automatically proved using the clauses in the program.
\end{enumerate}

The (operational) semantics of the connectives that occurs respectively in goal formulae $G$ / in HOHHF $H$ is given by the introduction/elimination rules of the connectives in natural deduction for intuitionistic higher order logic. In particular:
\begin{itemize}
\item the goal $H_1 \wedge H_2$ is turned into the two goals $H_1$ and $H_2$,
 copying the program (i.e. the set of clauses/assumptions)
\item the goal $H_1 \vee H_2$ is turned into the goal $H_1$; in case of failure of the proof search, one can backtrack and restart with the goal $H_2$
\item $H \Rightarrow G$ assumes $H$, that is turned into a new program clause, to prove $G$
\item $\forall x.G$ introduces a fresh variable $y$ and proves $G$ after replacing $x$ with $y$ in $G$
\item $\exists X.G$ introduces a fresh metavariable $Y$ and proves $G$ after replacing $X$ with $Y$ in $G$. Later $X$ can be istantiated with any term whose free variables were all in scope at the time $\exists X.G$ was processed
\item the goal $t$ where $t$ is atomic is proved unifying $t$ with the
(hereditary) conclusion of one of the program clauses, possibly opening new
goals as well (see case $G \Rightarrow H$ below)
\item assuming $H_1 \wedge H_2$ means assuming both $H$s independently
\item assuming $\forall X.H$ means assuming an infinite number of copies of $H$ for all fresh metavariables $Y$ substituted for $X$ in $H$. Concretely, the copy and substitution is performed lazily when the clause is used
\item assuming $\exists x.H$ means generating a new fresh constant $y$ and assuming $H$ after replacing $x$ with $y$
\item assuming $G \Rightarrow H$ means assuming that $H$ holds under the assumption that $G$ holds as well. When the clause is used, the new goal $G$ will be added to the set of open goals to be proved
\end{itemize}

In concrete syntax we will write \verb+sigma X\t+ for $\exists X.t$, \verb+pi x\t+ for $\forall x.t$, \verb+t_1 => t_2+ or \verb+t_2 := t_1+ for $t_1 \Rightarrow t_2$, \verb+t1,t2+ for $t_1 \wedge t_2$ and \verb+t1;t2+ for $t_1 \vee t_2$. Moreover, all free uppercase/lowercase variables in a program clause are implicitly universally/existentially quantified globally/locally (like in Prolog).

\texttt{sigma} in clause positions are not in ``standard'' LambdaProlog and they are accepted by our ELPI interpreter, but not from Teyjus. However, Teyjus implements a module system~\cite{???} that allows to declare variables local to the list $H_1,\ldots, H_n$ of clauses of the module. The same behaviour can be obtained in ELPI with (\texttt{sigma x \ }$H_1, \ldots, H_n$) or with the equivalent
syntactic sugar \texttt{\{ local x.} $H_1.\;\ldots\; H_n.$ \texttt{\}} (where the curly braces are used to delimit the scope of the existential quantifier, i.e. the local declaration). Declaring local constants is the only and fundamental mechanism in LambdaProlog to restrict the trusted code base.

The other major difference between the ELPI version of LambdaProlog and the ``official'' one of Teyjus is that we do not enforce any type discipline, nor we try to statically enforce the restriction to HOHHF. Errors at run-time (e.g. invoking a list as a predicate) result in abortion of the program execution. On the other hand, it is for example possible to read from the terminal a query and execute it, which is prohibited by Teyjus because the correctness of the query cannot be statically enforced.

To illustrate a typical LambdaProlog example, in Table~\ref{type-checker1} we show the code of a type-checker for closed terms of the simply typed lambda-calculus. We use the infix constant \verb+'+ to encode application, the unary constant \verb+lam+ for lambda-abstraction, and the infix constant \verb+-->+ for the function space type. The latter is to be applied to a meta-level lamdba-abstraction in the spirit of higher-order abstract syntax. For example, the term $\lambda x.xx$ is encoded as \verb+lam x \ x ' x+. Note that, in the concrete syntax, parentheses around lambda-abstractions are not necessary in LambdaProlog when the abstraction is the last argument of an application. I.e. \verb+lam x \ x ' x+ is to be read as \verb+lam (x \ x ' x)+.

Observe that the code of the type-checker is really minimal and in one-to-one correspondence with the two derivation rules one writes usually on paper. However, there are some differences as well. In particular, the typing judgement \texttt{term T TY} (``\texttt{T}'' is a term of type ``\texttt{TY}'') does not mention any context $\Gamma$. Instead, in the rule for lambda-abstraction, the hypothesis that the fresh variable \texttt{x} has type \texttt{A} is directly assumed and put temporarily in the program code using logical implication \texttt{=>}.
For example, the query \texttt{term (lam y \ y ' y) TY} will match the second
clause and trigger the new query \texttt{term (x ' x) B} after having istantiated \texttt{TY} with \texttt{A --> B} where \texttt{x} is fresh (because introduced by \texttt{pi}) and it is known to have type \texttt{A}.

Finally, we will use the cut predicate of Prolog \texttt{!} with the same
non-logical semantics. We say that a predicate does not have a logical semantics
when it breaks the commutativity rule of conunction or, equivalently, commutation of the semantics with istantiation.

\begin{table}[t]
\begin{small}
\begin{Verbatim}[numbers=left,numbersep=1pt,frame=leftline]
term (M ' N) B :- term M (A --> B), term N A.
term (lam F) (A --> B) :- pi x\ term x A => term (F x) B.
\end{Verbatim}
\end{small}
\caption{\label{type-checker1}A type-checker for simply typed lambda calculus.}
\end{table}

\section{HOL in a nutshell and its implementation in ML}
HOL is a variant of Church's simple theory of types. Its precise syntax and semantics can be found in~\cite{the_HOL_system_LOGIC} even if we will implement a constructive variant of it, and we will refer for inspiration to the HOL Light version and implementation of a classical variant of HOL. Types and terms have the
following syntax:
$$\begin{array}{l}
T ::= \alpha ~|~ u~T_1\ldots T_n ~|~ T \rightarrow T\\
t ::= x_T ~|~ c_T ~|~ \lambda x_T. t ~|~ tt
\end{array}$$
where $u$ ranges over type constructors (constants when $n=0$) and $\alpha$
ranges over type variables. Contrarily to System-F~\cite{???}, HOL has no type
abstractions/applications. However, it is possible to assign to constants
schematic types containing type variables $\alpha, \beta, \ldots$. For example,
the append function \texttt{@} can be typed as $\mathtt{list}~\alpha \rightarrow \mathtt{list}~\alpha \rightarrow \mathtt{list}~\alpha$.

Variables $x_T$ and constants $c_T$ carry explicitly their type $T$ and they are equal iff both the name and the types are. In particular, each occurrence of a polymorphic constant, carrying its type, explicitly carries an instantiation for every type variable occurring in its type. For example
\texttt{@}$_{\mathtt{list}~\mathtt{nat} \rightarrow \mathtt{list}~\mathtt{nat} \rightarrow \mathtt{list}~\mathtt{nat}}$ implicitly encodes the instantiation $\alpha := \mathtt{nat}$. In System-F the constant occurrence would be applied to the type: $\texttt{@}~\mathtt{nat}$.

HOL systems assume the existence of two predefined types, one for propositions \texttt{bool} and one for individuals \texttt{ind} (also written $o$ and $\iota$ in the litterature). Logical connectives and quantifiers are just single out constants. For example, conjuction is given the monomorphic type $\mathtt{bool} \rightarrow \mathtt{bool} \rightarrow \mathtt{bool}$ and universal quantification the polymorphic type scheme $(\alpha \rightarrow \mathtt{bool}) \rightarrow \mathtt{bool}$. The usual trick of higher-order syntax is used to reduce all binders to $\lambda$-abstraction: $\forall x:\mathtt{nat}.P$ is represented as $\mathtt{forall}_{(\mathtt{nat} \rightarrow \mathtt{bool}) \rightarrow \mathtt{bool}} (\lambda x.P)$. In the case of the HOL Light variant, the only constant that is initially assumed to exist is polymorphic equality $\mathtt{=}_{\alpha \rightarrow \alpha \rightarrow \mathtt{bool}}$, that also doubles for coimplication when $\alpha = \mathtt{bool}$.

The derivation system of the logic is made of two judgements, one for typing terms and the other for proving formulae in contexts (see ~\cite{???} for the XX derivation rules of the HOL Light implementation). Two examples of such rules are

\AxiomC{$\vdash p : \mathtt{bool}$}
\AxiomC{$\Gamma_1,p \vdash q$}
\AxiomC{$\Gamma_2,q \vdash p$}
\RightLabel{\tiny DEDUCT\_ANTISYM\_RULE \label{krule}}
\TrinaryInfC{$\Gamma_1,\Gamma2 \vdash p = q$}
\DisplayProof

\AxiomC{$\Gamma \vdash p$}
\RightLabel{\tiny INST\_RULE}
\UnaryInfC{$\Gamma \sigma \vdash p \sigma$}
\DisplayProof

Like in the HOL Light implementation, the check for $p$ being a boolean in the former rule can be post-poned to the time the hypothesis is used (the HYP rule, here not shown).
The latter rule is used to apply the substitution $\sigma$ that maps free variable names to terms. Its application must avoid variable capturing.

An HOL theory is a list of declarations of types, constants and axioms, the latter being just formulae assumed to hold.

The implementation of an HOL systems is rooted in the principle of conservative extensions: in order to introduce a new constant or a new type that satisfy certain axioms, the user must prove the existence of such a constant/type. We only show here the rule for introducing new definitions to explain a kind of complexity that involves free and bound names. 

To introduce a new (polymorphic) constant $c_T$, the user must give a term $t$
of type $T$ such that all free type variables contained in $t$ also occur in
$T$. After checking the conditions and veryfing that no constant named $c$ is already defined in the current theory, the latter is extended with the new
constant $c_T$ and the axiom $c_T = t$.

The condition on the free type variables of $t$ avoids logical inconsistency
and would not be necessary in System-F where every occurrence of $c$ would be explicitly applied to every free type variable that occurs in $t$. In HOL, instead, only the type instantiations for the type variables that occur in the type of $c$ can be recovered from the type of $c$. Therefore, if the condition is violated, $c_T$ could be rewritten via the axiom to an instantiation of $t$ that still contains free type variables. Example: $c_\mathtt{bool} = ! x_\alpha y_\alpha \  x_\alpha = y_\alpha$ that from $c_\mathtt{bool} = c_\mathtt{bool}$ via two instantiations would imply $! x_\mathtt{bool} y_\mathtt{bool} \  x_\mathtt{bool}= y_\mathtt{bool} \iff ! x_\mathtt{unit} y_\mathtt{unit} \  x_\mathtt{unit}= y_\mathtt{unit}$ i.e. $\mathtt{false} = \mathtt{true}$.

\subsection{HOL (Light) in ML}
HOL Light~\cite{??} is an extremely lightweight implementation of HOL in ML that reuses the ML toplevel, following the tradition started by Milner~\cite{???}. The key idea is the possibility in ML of encoding \emph{private data types}~\cite{??}, i.e. data types whose values can be inspected via pattern matching outside the module that defines them, but that cannot be directly constructed outside the module. Instead, from outside the module, it is only possible to invoke a function, implemented in the module, that returns an inhabitant of the type.

Private types can be used to encorce invariants: the functions of the module only builds terms that satisfy the invariants. Therefore, all inhabitants built outside the module are correct by construction, and the trusted code base for the maintainance of the invariants is lexically delimited by the module.

HOL Light implements two private types corresponding to the two judgement of the logic: 1) the type of well typed terms $t$ such that there exists a $T$ s.t. $\vdash t : T$; 2) the type of theorems, i.e. pairs context $\Gamma$, formula $F$ such that $\Gamma \vdash F$. Theories could also have been implemented as a third private type (like what Coq does), but they are actually hidden inside the module that define the type of (well typed) terms.

In particular, every inference rule of HOL corresponds to a function over theorems exported from the ``kernel'' module of HOL.

\noindent
\verb+val mkComb : term -> term -> term+\\
\verb+val DEDUCT\_ANTISYM\_RULE : thm -> thm -> thm+\\
for example correspond respectively to the typing rule for application $t_1 t_2$ and to the inference rule shown on page~\ref{krule}. The function\\
\verb+val dest_thm : thm -> term list * term+\\
allows to inspect a theorem to recover what has been proved. Of course, it does not have an inverse function that would correspond to claiming that $\Gamma \vdash p$ is a theorem without actually proving it.

An important feature of this approach, that dates back to LCF and puts it apart from the Curry-Howard approach used for Coq and other systems for dependently typed lanugages, is that \emph{no proof object} need to be stored. Indeed, an inhabitant of \verb+thm+ really is (in HOL Light) just a pair hypotheses/conclusion.

The functions implemented in the kernel already allow to build proofs in bottom-up style. Quite frequently, however, the user would like to mix the bottom-up and the top-down styles, and he would like to be able to use metavariables for yet unknown formulae during a top-down proof.

Again following Milner's LCF, HOL Light implements outside the kernel a mechanism for top-down proofs based on the notion of tactic. Simplifying, the tactic type is defined as\\
\verb+type tactic =+\\
\verb+  sequent -> sequent list * instantiation * metavariables * justification+\\
\verb+type justification = instantiation -> thm list -> thm+

Ignoring the instantiation and metavariables, a tactic is a function that takes in input the sequent to be proved and return a list of new sequents to be proved and a justification. When the new sequents are proved, a list of theorems is obtained. The justification, fed with this list of theorems, returns the theorem for the initial sequent. In other words, each tactic implements a top-down reasoning step by producing the list of new subgoals plus the ``inverse'' bottom-up proof. When those are composed, the theorem is proved. Variables recorded as metavariables, i.e. omitted information, and instantiations for them complete the picture.

We make a few observations here:
\begin{itemize}
\item As we already noticed in~\cite{??}, LCF tactics have a few drawbacks. They only allow to reason locally on one goal, because they do not take in input the global proof state. In case of mutual dependencies between goals due to metavariables, it becomes impossible to reduce the search space using global reasoning. Moreover, it is not possible to interrupt the execution of a tactic in the middle, for example to observe the intermediate states. The tinycals introduced in~\cite{??}, which can now be implemented in Coq as well after~\cite{??}, are a better solution but they require a first class representation of the entire proof state.
\item During a top-down proof, the system temporarily records a proof object in the form of a justification, that is lazily consumed as soon as the corresponding subproof is completed (all leaves are closed). We would like to avoid this if possible.
\item Every bottom-up inference rule --- primitive or derived --- performs some checks that are performed twice when the rule is turned into a top-down tactic.
For example, the top-down tactic for the \verb+DEDUCT\_ANTISYM\_RULE+ rule must analyse the sequent in input to verify that it is of the form $\Gamma \vdash p=q$, yielding the two new goals $\Gamma, q \vdash p$ and $\Gamma, p \vdash q$. Later the justification \verb+DEDUCT\_ANTISYM\_RULE+ needs to pattern match the list of theorems in input against $\Gamma_1,q \vdash p$ and $\Gamma_2,p \vdash q$. The ``duplicate'' inverse checks are necessary because the kernel does not trust the outer parts of the system, but are still somehow undesired.
\end{itemize}

\section{Requirements and Term Encoding}
We begin the description of our implementation of (a constructive variant of)
HOL in ELPI, our LambdaProlog interpreter. The code described here is available at~XXXXX. The implementation is meant as an experiment to evaluate if Higher Order Logic Programming can indeed be employed concisely and effectively for the implementation of interactive theorem provers. In this experiment, we take inspiration from HOL Light and we set the following requirements:
\begin{enumerate}
\item The system must have a small trusted code based, henceforth called the ``kernel''. Bugs outside the kernel must not allow to conclude that a non-tautological formula $F$ is a theorem.
\item The LCF architecture does not need to record proof objects. Therefore we do not want to record proof objects as well.
\item The implementation must use genuine Higher Order Logic Programming techniques. Binders have to be handled exploiting the binding machinery of the language. Object level metavariables must be represented by the metavariables of the language.
\item Tactics must be able to inspect the global proof state.
\end{enumerate}

The third requirement rules out the encoding of the HOL Light in LambdaProlog. A mechanical encoding is possible because the language is Turing complete, but it would not benefit from the features of the language.

The second and fourth requirement rule out the technique used by Amy Felty and, more recently, by the group of Dale Miller~\cite{???}. Felty and Miller encodes the judgements of the object logic via a LambdaProlog predicate that defined by recursion over a proof object (Felty) or a proof certificate (Miller).
For example, the proof object can decide to apply the transitivity rule for application to a goal where multiple rules apply. A proof certificate is similar to a proof object, but it can avoid recording all the needed information or encode it at a different level of detail. Therefore, a proof certificate may be non syntax directed, in the sense that multiple rules can be tried according to the same certificate, triggering backtracking if needed.

In both cases, the system can be simply turned into both an automatic or an interactive theorem prover. In the first case, it is sufficient to replace the certificate with a metavariable: the LambdaProlog predicate will then work in a generative way, blindly enumerating all possible proof objects (and potentially diverging because of depth-first search, that can be replaced with iterative deepening to keep completeness). To obtain an interactive prover, the certificate is replaced with the request to the user to type the next rule to be tried. The user interaction can also print the sequent to be proved, but not the other open branches of the proof, that are kept at the meta-level in the and/or tree of the LambdaProlog interpreter.

Finally, we cannot implement the kernel using a private type like in ML because Logic Programming rules out private types. To understand why, remember that every predicate in a (pure) logic programming language is invertible. Therefore it is not possible to implement a type with a predicate to reveal the data, but preventing its use to build the data as well. In particular, the predicate \verb+dest\_thm+ corresponding to the function with the same name in HOL would allow to feed it any pair context-formula to obtain a theorem without any further control.

The only way to protect part of the code in LamdaProlog is to existentially quantify a predicate to be kept secret to the rest of the code. Therefore, our kernel implements two predicates, one for the typing judgement and one for derivability, that are kept local. In order to fullfill requirement number 4, derivability will work on list of sequents (the proof state) in place of a single sequent. Finally, to avoid the repetition of code and checks in the HOL approach, we only focus for now on top-down proofs.

We detail now the implementation of the kernel.

\subsection{Semi-shallow encoding of terms}
Before starting, we first need to choose a syntactic representation for both
terms and types that respect requirement 3, that rules out deep encodings.
A deep encoding for simply typed $\lambda$-calculus with metavariables would
look like this and be very similar to the data type Coq and Matita are build around:

$$ t ::= var~\mathtt{NAME} ~|~ app~t~t ~|~ lam~\mathtt{TYPE}~\mathtt{NAME}~t ~|~ meta~\mathtt{NAME}$$

Assuming $\mathtt{NAME}$s to be either strings or De Brujin indexes, a deep encoding forces the implmenetation of $\alpha$-conversion, lifting, substitution, instantiation of metavariables, etc.

At the other side of the spectrum we find the shallow encoding for closed terms: we just encode object level terms with language level terms, object level metavariables with LambdaProlog metavariables and so on. Instantiation of metavariables as well as $\alpha$-conversion and $\beta$-reduction comes for free. However, it becomes impossible to implement --- without extra-logical extensions of the language --- both the derivation and the typing judgements. For example, the code
\verb+type (F X) B :- type X A, type F (A --> B)+ does not do what it seems to
do, because applied to the query \verb+type (f a) Z+, it yields the higher order
unification problem \verb+F X = f a+ that admits multiple solutions like
\verb+F := x \ f a+, \verb+X := 0+.

We therefore propose what we call the \emph{semi-shallow} encoding:

$$t ::= t~\verb+'+~t ~|~ lam~\mathtt{TYPE}~F$$

where \verb+'+ is just an infix constructor, \verb+F+ is a LambdaProlog function from terms to terms and object level metavariables are encoded via language level metavariables. For example, $\lambda x\!:\!o. X~x$ is encoded as
\verb+lam o x \ X x+. Ignoring the issue about metavariables, the semi-shallow encoding was the one already used in Table~\ref{type-checker1}, that essentially show the typing judgement of our implementation of HOL Light, which includes a few other clauses for primitive constants, like \verb+term eq (A --> A --> prop)+.

For simply typed $\lambda$-calculus (plus type variables) type checking is decidable and the typing judgement of Table~\ref{type-checker1} is already syntax directed. Therefore HOL never asks the user to prove that a term has a certain type: type checking/inference is run fully automatically.

Because we can inject LambdaProlog variables in place of types of bound variables, our type checking judgement already doubles as type inference. E.g. \verb/term (lam A x \ x + 1) B/ yields \verb+A := nat, B := nat+.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Towards a Constrained Programming Extension of $\lambda$Prolog}
\label{cholp}

The prover has to be able to apply the typing judgement to incomplete terms,
i.e. terms that contain metavariables to be later instantiated by the
user.  If such judgement is expressed as a regular logic program the choice
of using $\lambda$Prolog metavariables to represent object-level metavariables
may causes serious issues, since logic programs do not normally distinguish
between input and output, and hence could generate the term in place of the
user.  Take for example the \verb+term+ clause for lambda and imagine
it is run on an incomplete term \verb+(lam nat n\X n) T+

\begin{verbatim}
term (lam A F) (A --> B) :- pi x\ term x A => term (F x) B
\end{verbatim}

By applying such clause one would end up with the goal \verb+term (X n) B+
under a program augmented with the clause \verb+term n nat+.  Such
clause, as well as any other one for \verb+term+ would apply.  As a result
the meta variable \verb+X+ would be instantiated by \verb+n+, or as an
alternative solution \verb+(lam ..)+.

What provers implemented in ML, like Coq or Matita, do is to keep
a data structure that assigns a type to each metavariable, typically called
$\Sigma$.  When an incomplete term is found the algorithm stops and check
that the (occurrence of) the metavariable has the type declared in $\Sigma$ (1).
In a different part of code, when a metavariable is instantiated, the system
checks that the instantiation is compatible with type declared in $\Sigma$ (2).

The reading of this behavior in terms of logic programming points toward two
well known mechanisms.  

First, a goal like \verb+term (X n) B+ has to be
\emph{delayed}, a very well known mechanisms justified by the commutativity of
conjunction: postponing a goal does not change the meaning of the program.
Actually one can see the $\lambda$Prolog goals as a set of \emph{constraints},
all to be solved at some point.  Most Prolog system provides such mechanism.
Teyjus has it too but only for internal use, i.e. delay hard HO unification
problems falling outside the L$_\lambda$ fragment.  Goals are delayed on
a specific metavariable and are re-scheduled when such metavariable
gets instantiated.  This mechanism avoids entering generative mode and
faithfully models (2).

Last, when two goal, i.e. constraints, are delayed on the same meta variable
one could add to the set of goals an equation between the two types.  The
manipulation of the set of delayed goals is again a mechanism that accompanies
many Prolog implementations in the for of \emph{constrain handling rules},
or CHR.  Teyjus does not provide such mechanism.

At the light of that we augmented ELPI with these two mechanism letting us
express the typing judgement as follows

\begin{verbatim}
mode term(-,+) {
  term (M ' N) T :- term M (A --> B), term N A.
  term (lam A F) B :- pi x\ term x A => term (F x) B.
  term eq (A --> A --> prop).
  % pick your favourite
  term (? as K) T :- constraint (term Key T) K.
  term (? as K) T  as Goal :- constraint Goal K.
}
\end{verbatim}

The \verb/mode term(-,+)/ block declares that \verb+term+
is never generative on the first argument.  The last line uses the \verb+?+
keyword, only available in a \verb+mode+ block, to recognize
flexible arguments.  The \verb+as K+ is just sugar reminiscent of ML
pattern matching to name a sub expression expression, in this case
the metavariable.  \verb+constraint+ declares a new constraint.
Note that also dynamic clauses, like the \verb+term x A+ in the
\verb+lam+ rule lives in the \verb+mode+ block and hence
consumes, never generates, its first argument.

As a companion mechanism we provide the possibility to declare constrain
handling rules

\begin{verbatim}
constraint term {
  rule [ term (? as X) T1 ] [ term (? as Y) T2 ] (T1 = T2) :- X = Y.
}
\end{verbatim}

The syntax departs a little from the standard CHR one.  The first two
arguments are lists of goals, the second one to be removed.  The third argument
is a goal to be added and the premised of the rules is the guard. The rule
above hence reads: if two \verb+term+ constraints are on the same meta variable,
remove the second one and add a goal \verb+T1 = T2+.

Rules like this one are typically justified by the meta theory of the
object level language.  In particular this one corresponds to the uniqueness
of typing.

\subsection{HO extensions (maybe in the conclusions)}

$\lambda$Prolog goals live in a context (the dynamic part of the program)
that also has to be made available to
the constraint handling rules.  For example one may check that not only the
instance of meta variables are well typed, but also that all occurrences of the
metavariable itself are (makes sense?).

\begin{verbatim}
constraint term ?- term {
  rule [ G1 ?- term (?K1 L1) _ ] [ G2 ?- term (?K2 L2) _ ] CompCtx :-
    K1 = K2, aux L1 G1 L2 G2 CompCtx.

  aux [X|XS] G1 [Y|YS] G2 (TX = TY, CompCtx) :-
    % requires simple clausify fix
    (G1 => term X TX), (G2 => term Y TY), aux XS G1 YS G2 ComCtx.
  aux nil nil nil nil true.
  aux _ _ _ _ false.   
}
\end{verbatim}

For both performance and usability reasons one has to specify
which components of a program context he is interested in.  Hence the
\verb+term ?-+ part saying that the context items we are interested in
are only the ones with \verb+term+.  Talk about nabla?

% Work in this direction is in progress. As a first step that does not involve constraint propagation, we would like to specify that a syntax directed predicate is to be turned into a constraint when it faces a metavariable. Hypothetical syntax: \verb+delay (term X T) until not flexible X.+ For the time being, we encode the syntax above with two low-level primitives not in standard LambdaProlog:
% \begin{enumerate}
% \item \verb+$is_flex X+ holds iff \verb+X+ is a flexible term, i.e. a metavariable or an application of a metavariable.
% \item \verb+$delay G X+ always holds, putting the goal \verb+G+ into a list of delayed goals, to be resumed as soon as the metavariable \verb+X+ is instantiated.
% \end{enumerate}
% 
% With these two primitives, turning \verb+term+ into a constraint on flexible terms can be encoded as in Table~\ref{type-checker2}.
% \begin{table}[t]
% \begin{small}
% \begin{verbatim}
% term T TY :- $is_flex T, !, $delay (term T TY) T.
% term T TY :- term' T TY.
% term' (lam A F) (A --> B) :- pi x\ term' x A => term (F x) B.
% term' (F ' T) B :- term F (A --> B), term T A.
% term' eq (A --> A --> prop).
% \end{verbatim}
% \end{small}
% \caption{\label{type-checker2}A type-checker for simply typed lambda calculus in Constraint Programming style.}
% \end{table}
% 
% The first line delays the goal \verb+term T TY+ iff \verb+T+ is flexible. Otherwise the second line immediately turns it into \verb+term' T TY+. The next lines are the old implementation of \verb+term+ where \verb+term'+ is used in place of \verb+term+ in the heads of the clauses. The reason for the introduction of \verb+term'+ is that the new clauses \verb+term' x A+ introduced in the body of the lambda-abstraction case must be used strictly after the first, delaying clause. According to the standard semantics of LamdaProlog, a clause introduced by logical implication is always tried first. Hence the need for the technical \verb+term/term'+ distinction.
% 
% The \verb+$is_flex+ predicate is clearly extra-logical (it does not commute with instantiation). However, the high-level programming construct \verb+delay+ \ldots \verb+until not flexible+ that we have just encoded is perfectly logical.
% 
% We also have an experimental, CHR-inspired syntax to propagate constraints, but we omit its explanation from the paper because it is still work in progress. We only note here that propagation is a logical operation iff the propagation rule can be proved to holds as a meta-theoretical theorem. Therefore, so far, the proposed extensions are fully in the spirit of logic programming.

\section{The kernel in LamdbaProlog}
The second judgement implemented in our kernel is the derivation judgement in top-down style. It is encoded via three predicates, \verb+prove+, \verb+loop+ and \verb+thm+.

The predicate \verb+loop SEQS CERTIFICATE+ holds when the list of well-typed sequents \verb+SEQS+ can all be proved following instructions from the certificate \verb+CERTIFICATE+.

The predicate \verb+prove G CERTIFICATE+ holds when the formula \verb+G+ is well-typed of type \verb+prop+ and moreover \verb+loop [(seq [] G)] CERTIFICATE+ holds, where \verb+seq [] G+ represents a sequent whose context is empty and whose conclusion is the closed formula \verb+G+.
\begin{small}
\begin{verbatim}
prove G TACS :-
 (term G prop, ! ; parse PG G, $print "Bad statement:" PG, fail),
 loop [ seq [] G ] TACS.
\end{verbatim}
\end{small}
The code executed when type-checking fails provides a nice error message to the user. We call the type of propositions \verb+prop+ in place of \verb+bool+ because our logic is intuitionistic.

The code for \verb+loop+ is the following:
\begin{small}
\begin{verbatim}
loop [] CERTIFICATE :- end_of_proof CERTIFICATE.
loop [ SEQ | OLD ] CERTIFICATE :-
 next_tactic [ SEQ | OLD ] CERTIFICATE ITAC,
 thm ITAC SEQ NEW,
 append' NEW OLD SEQS,
 update_certificate CERTIFICATE ITAC NEW NEW_CERTIFICATE,
 loop SEQS NEW_CERTIFICATE.
\end{verbatim}
\end{small}
The predicate calls the three predicates \verb+end_of_proof, next_tactic, update_certificate+ that are untrusted and defined outside of the kernel. Their intended usage is to extract from the certificate the next rule to use (a primitive inference rule or a user-defined tactic), and to update the certificate according to the new proof state. They are closely reminiscent of the corresponding ones in~\cite{miller????}.

Predicate \verb+loop+ succeeds when there are no more sequents to prove, informing the certificate via \verb+end_of_proof+ (or verifying that the certificate agrees). Otherwise the predicate feeds the global proof state (the list of sequents) to \verb+next_tactic+ that, according to the certificate and the proof state, returns a rule \verb+ITAC+ to apply.

Then \verb+loop+ invokes \verb+thm ITAC SEQ NEW+ that applied the rule \verb+ITAC+ to the goal \verb+SEQ+, reducing in top-down style a proof of it to a proof of the list \verb+NEW+ of new sequents (the hypotheses of the rule). Each primitive inference rule of HOL (Light) is implemented by \verb+ITAC+. For example, \verb+DEDUCT\_ANTISYM\_RULE+ (that we simply cal \verb+k+) is implemented by

\verb+thm s (seq Gamma (eq ' P ' Q)) [ seq (P :: Gamma) Q, seq (Q :: Gamma) P ] :- reterm P prop.+

As an invariant, we already know that \verb+eq ' P ' Q+ is well typed. The \verb+reterm P prop+ judgement is a variant of \verb+term+ that performs less checks knowing that \verb+P+ is already well typed. Note that, of the three premises of \verb+DEDUCT\_ANTISYM\_RULE+, the typing one is automatically discharged because retyping is decidable and efficient. The same happens in HOL Light.

Once \verb+thm+ returns, the list of new sequents to prove is inserted in the
proof state and the certificate is updated via \verb+update_certificate+ before entering the next loop iteration.

All predicates involved but \verb+prove+, i.e. \verb+loop,thm,term,term',reterm+ are declared \verb+local+ to the kernel and the code is written carefully to avoid leaking them. Indeed, if such a predicate leaked out, it would be possible for a malicious user to augment the clauses about the predicate, breaking logical correctness (e.g. executing \verb+thm [seq [] false] [] ==> prove false X+). The problem of avoid leaking is clearly undecidable, but a simple static analysis can verify that our kernel does not leak (essentially because the predicates are only used in the kernel in head position and never in argument position and so they cannot be leaked out). The static analysis has not been implemented yet.

Compared to the HOL Light kernel, our kernel verifies whole proofs, whereas HOL Light only verifies the application of a single inference rule. The reason is that, because of the private type implementable in ML, composing inference rules outside the kernel via function application is a safe operation in ML. In our case, without the private type, composition of rules must be fully implemented inside the kernel.

\subsection{Free names in sequents}
In HOL sequents can contain free variables of any type, and the \verb+INST\_RULE+ allows to later instantiate them freely. In LambdaProlog we want to work only on closed terms in order to reuse the language binding mechanism to avoid implementing $\alpha$-conversion, instantiation, etc. Some inference rules, however, do introduce fresh names (typical examples are the introduction rule for the universal quantifier). Because free variables are not allowed in the semi-shallow encoding, we need to find a representation of sequents where all variables are bound ``outside the sequent''. The one we chose puts enough \verb+bind TYPE+ binders around the sequent. For example, the primitive congruence rule for $\lambda$-abstractions is written as:

\begin{small}
\begin{verbatim}
thm k (seq Gamma (eq ' (lam A S) ' (lam A T)))
 [ bind A x \ seq Gamma (eq ' (S x) ' (T x)) ].
\end{verbatim}
\end{small}

Correspondingly, we introduce two new primitive rules to prove sequents
that have free names:

\begin{small}
\begin{verbatim}
thm (bind A TAC) (bind A SEQ) NEWL :-
 pi x \ term' x A => thm (TAC x) (SEQ x) (NEW x), put_binds' (NEW x) x A NEWL.

thm ww (bind A x \ SEQ) [ SEQ ].
\end{verbatim}
\end{small}

Rule \verb+ww+ (for weakening) is easier to understand: if \verb+x+ does not occur in the sequent, proving \verb+bind A x \ SEQ+ is equivalent to proving \verb+SEQ+.

The first rule says that to prove a \verb+bind A x SEQ+ using a rule \verb+bind A TAC+ which is itself parametric on a variable having the same type, it is sufficient to pick a fresh name \verb+x+, assume that is has type \verb+A+, and then call rule \verb+TAC x+ (the rule ``instantiated'' on the fresh name) on \verb+SEQ x+ (the sequent ``instantiated'' on the fresh name). The result will be a list of new sequents \verb+NEW x+ where \verb+x+ can occur. The predicate \verb+put_binds'+ (code of 4 lines not shown for space reasons) prefixes every sequent in the list \verb+NEW x+ with the \verb+bind A x \+ binder.

Compared to HOL Light, these two rules are new, but \verb+INST\_RULE+ (and the corresponding one for type variable instantiation) are no longer necessary. Moreover, the remaining primitive rules are slightly simplified by removing $\alpha$-equivalence tests where they occur in HOL Light code.

\subsection{Tactics as macros}
A rule in the previous paragraphs is meant to be either a primitive rule of the logic, or a user-defined tactic. In LCF tradition a tactic takes in input a local proof state and applies a bunch of other rules, either primitives or tactics.

In our code, a tactic is essentially a user-defined macro that expands to the composition of other rules. The expansion is determined by the tactic parameters but also by the (local) proof state. The invocation of the expansion is implemented in the kernel, but the expansion predicate \verb+deftac+ is untrusted and freely implemented outside.

\begin{small}
\begin{verbatim}
thm TAC SEQ SEQS :- deftac TAC SEQ XTAC, thm XTAC SEQ SEQ.

% Example outside the kernel
deftac forall_i (seq Gamma (forall '' _ ' lam _ G)) TAC :-
 TAC = then (conv dd) (then k (bind _ x \ eq_true_intro)).
\end{verbatim}
\end{small}

Note how the \verb+forall\_i+ tactic for the introduction of universal quantifiers is implemented composing other tactics (\verb+eq\_true\_intro, k+ and the conversion \verb+conv dd+) via LCF tacticals (\verb+then, thens,+ \ldots). Note also that, contrarily to every other interactive theorem prover out there, our scripts containts binders for the names of freshly introduced variables. For example, as we have seen in Section~\ref{???} the \verb+k+ rule above introduces a fresh name, and the tactic definition must start binding the name and retrieving its type (via \verb+bind _ x+). The name can then be used in the rest of the script if necessary.

The use of binders in the scripts is a clear improvement over other systems, where just strings are used to refer to the name of hypotheses, with major problems in case the names change, e.g. because of a new version of the system. Proof refactoring~\cite{???} should also be simplified.

A tactical \verb+bind* TAC+ is also provided to bind-and-ignore as many fresh names as possible, that are not bound in \verb+TAC+ that therefore cannot use them. It is necessary, for example, after the application of the \verb+repeat+ tactical to a tactic that generates fresh names, because it is statically unknown how many fresh names will be generated.

For the time being we do not assign names to hypotheses, like in HOL Light and unlike other theorem provers like Coq. Adding names to hypotheses via binders in the script as we did for fresh names does not introduce any new difficulty.

\subsection{Tacticals}
An LCF tactical is an higher order tactic. Most tacticals starts applying a tactic and then continue on the resulting subgoals. Tacticals can be defined in ML outside the kernel by inspecting the result of the first tactic application and using safe function composition. We are forced again to provide a kernel-side mechanism to implement tacticals as we did for tactics. It boils down to the new following primitive rules:

\begin{small}
\begin{verbatim}
thm (thenll TAC1 TACN) SEQ SEQS :-
 thm TAC1 SEQ NEW,
 deftacl TACN NEW TACL,
 fold2_append' TACL NEW thm SEQS.

thm (! TAC) SEQ SEQS :- thm TAC SEQ SEQS, !.

thm id SEQ [ SEQ ].
\end{verbatim}
\end{small}

The first rule apply a tactic \verb+TAC1+ and then asks the untrusted predicate \verb+deftacl+ defined in user space to expand the ``tactic'' (certificate?) \verb+TACN+ to a list of tactics \verb+TACL+. The expansion can observe the list of goals generated by \verb+TAC1+ and each new tactic will be applied to the corresponding new goal by \verb+fold2\_append+ that also collects the set of new, final goals.

Rule \verb+thenll+ already allows to implement in user space most LCF tacticals like \verb+then+ and \verb+thenl+. The remaining can be implemented using the \verb+id+ rule too, that is the neutral element of composition (it does nothing).

Finally, the extra-logical \verb+(! TAC)+ tactical applies the tactic \verb+TAC+ and then suppress all backtrackings via cut. It is very useful in those situations where the user whants to implement invertible tactics~\cite{????}, i.e. tactics that may be backtracked, but that do not change provability and thus should not be backtracked.

We observe that the tactics and tacticals that can be implemented in our system goes beyond the LCF ones because we inherit full-backtracking from LambdaProlog. In particular, our tactics are naturally non-deterministic, i.e. they can have multiple solutions (that are enumerated via backtracking).
The LCF version of the \verb+then+ tactical that never backtracks the first tactic corresponds to our restriction to banged tactics: \verb+then (! TAC1) TAC2+.

\subsection{Uses of the certificates}

We implemented outside the kernel two kind of certificates: interactive and non interactive. A non interactive certificate is just a proof script, i.e. a list (usually a singleton) of tactics to be applied sequentially to prove the open goals. Non interactive certificates are partially inspired by the work of Chihani~\cite{chihaniXXX}: they also consist in a list of tactics, but at the first call a singleton list containing a metavariable is passed. When the next tactic is required, the tactic is interactively asked to the user and recorded in the certificate by instantiating the metavariable. Moreover, if the tactic is an application of the \texttt{thenll} tactical, the argument of \texttt{thenll} is a list of fresh metavariables that will be instantiated later with the tactics used in any branch.

At the end of an interactive proof, the interactive certificate has been fully instantiated recording all tactics applied, and it becomes usable as a non interactive certificate (i.e. a proof script). To improve readability of the script, we wrote a normalization function that rewrites applications of \texttt{thenll} to application of \texttt{then} and \texttt{thenl} when possible. The normalized script is pretty-printed to the user that can copy\&paste it into the library.

\subsection{Final considerations}

As described in the previous sections, our kernel implements an higher number
of basic inference rules w.r.t. the HOL Light kernel. Nevertheless, as expected, our kernel is considerably smaller: 220 lines of self-contained LambdaProlog code vs 476 lines of OCaml code that also reuses standard library functions on lists. The greatest gain is due to saving most of the 228 lines of OCaml code that deal with instantiation, substitution, renaming and $\alpha$-conversion of terms in HOL Light: these operations all come for free in LambdaProlog thanks to the semi-shallow encoding.

\section{Towards More Extensions to LamdbaProlog}

In Section~\ref{cholp} we already discussed one of the drawbacks of the semi-shallow encoding: the need to extend the language with constraints in order to delay predicates --- like typing --- that are faced with metavariables that represent object level unknown terms.

A similar problem, which requires a different solution, occurs in two more places in the code. The first place is parsing/pretty-printing (implemented by one predicate that can be used in two different ways): we want to write a function that abbreviates some common expressions using an ad-hoc syntax. For example, the following clause allows for the compact syntax \texttt{! x \ P ' x} for universal quantification in place of the inner representation \texttt{forall '' T '' lam T x \ P ' x} (where \texttt{T} is the type of the bound variable that is omitted):
\begin{verb}
parse (! F2) (forall '' _ ' lam _ F1) :- !, pi x \ parse (F2 x) (F1 x).
\end{verb}

When the \verb+parse+ predicate is fed (on the left, for parsing, or on the right, for pretty-printing) with a term that contains an (object-level) metavariable, it must not instantiate it. However, the semantics of LambdaProlog would just start instantiating the metavariable enumerating all possible terms.

Therefore, we need a new mechanism to invoke a predicate specifying that the metavariables in one of the arguments are not to be instantiated or, equivalently, that backchaining over a clause must be performed up to matching only on the ``input'' argument.

An hypothetical syntax could be:

\begin{verb}
\$mode [ parse(-,+) ] parse t X  % for parsing
\$mode [ parse(+,-) ] parse X t  % for pretty-printing
\end{verb}

whose semantics is the follow: when backchaing over a clause of parse, \verb+t+
will be matched, while \verb+X+ will be instantiated. Moreover, the same ``mode'' will be inherited by any recursive call of \verb+parse+ triggered in the body of the clause.

Therefore, adding to \verb+parse+ the catch-all clause ``\verb+parse T T.+'', a metavariable will always be parsed/pretty-printed to itself.

The second place in the code where a similar problem arises is in the implementation of automation. An automatic tactic chooses the next tactic to apply by inspection of the proof state, and it also has to determine if the proof step is invertible or not. In the former case, it does not change provability and therefore there is no necessity to backtrack it in case of failure.

Without the possibility to inspect terms via matching only to detect metavariables in the sequent, it is impossible to determine invertibility of rules. For example, the rule ex-falso allows to prove any sequent $\Gamma \vdash P$ where
$\bot \in \Gamma$ and it is an invertible rule. Therefore, adding a cut \texttt+!+ to the tactic invocation is desirable. However, it may be the case that
a metavariable $X$ occurs in $\Gamma$. When LambdaProlog backchains over ex-falso, it will instantiate $X$ with $\bot$ solving the goal. The instantiation itself is not invertible, for example, when $X$ occurs in another goal $\vdash X$ that becomes unprovable.

The solution to this and a few more similar examples consists again in backchaining using matching in place of unification for $\Gamma$ and it is therefore supported by our proposed language extension.

As a final consideration, and contrarily to the extensions with constrains (and constrain propagation), the matching extension is not logical: matching does not commute with instantiation. This is the price to pay for the benefits of the semi-shallow encoding.

\section{User Interface Issues}
\subsection{Representation of Terms and Proof States}
The internal representation of terms in an interactive prover is often very verbose and it is customary to allow to write terms using a so called ``outer'' syntax that allows for infix operators, mathematical notations and omission of information (e.g. types). In order to let the users extend the system easily, the outer notation should be also available when defining tactics.

HOL Light is the system with the best support of outer syntax, even when defining tactics. This is achieved by means of a pre-processor for OCaml (written in camlp5~\cite{?}): all strings written in backquotes in the code of HOL Light are parsed and type-checked by the pre-processor and turned into well-typed terms. The opposite is done during printing. The code of the pre-processor amounts to 2845 lines. When an user develops a library, she does not need to write any additional parsing code, but she can always briefly define a new operator as prefix/infix/etc.

In our prototype the user can directly write terms written in our semi-shallow encoding. For example, $P x \wedge Q$ should be written as \texttt{and ' (P ' x) ' Q}. To reduce the burden on the user, we provide the parse/pretty-printing predicate discussed in the previous Section. Contrarily to HOL Light, the outer syntax is not simply a string, but it is again a structured term that extends the one of the internal syntax. Infix binary operator and prefix/postfix unary operators can be directly used in the external syntax because LambdaProlog itself allows to declare constants as such. Thus, for example, \texttt{and ' (P ' x) ' Q} can be abbreviated to \texttt{P ' x \&\& Q} but not to \texttt{P ' x /\\ Q} because \texttt{/\\} is not a valid LambdaProlog constant. Moreover, infix ticks must still be used to separate between function arguments.

A second difference  is that terms in the ``outer'' syntax can contain metavariables, allowing interpolation of (meta)variables for free like in \texttt{p X :- parse (X \&\& X) O, \$print O}. However, when used without care interpolation is computationally expensive. For example, the previous clause would interpolate \verb+X+ in \verb+X \&\& X+ before calling \verb+parse+ that will therefore traverse \verb+X+ twice. A better implementation would be \texttt{p X :- parse (Y \&\& Y) O, parse X Y.}.

A third difference is that the terms of our syntax does not admit free variables: all variables have to be quantified, either inside the term, or outside it in the code. For example, where a HOL Light user would state the polyjorphic theorem \verb+ prove `a = b => b = a`+ our user must state \verb+theorem (pi A \ ! A a \ ! b \ a = b => b = a)+ where the two variables are universally quantified in the statement and the abstraction over the type $A$ is quantified outside it to make the theorem polymorphic.

The last, and most annoying, difference is that the parsing predicate must be extended by the user every time it declares a new tactic whose syntax can contain a term, e.g. like in \texttt{parsetac (t Y) (t PY) :- parse Y PY.} where \verb+Y+ is a term argument of the tactic \verb+t+.

All the above observations suggest that a better approach would be closer to the one of HOL Light: we could integrate in ELPI a pre-processor to detect and handle terms during the parsing and pretty-printing phase.

In the semi-shallow encoding, bound variables are represented by variables bound at the metalevel. In non-interactive scripts, the binders are visible and the user can pick any name for the variables. However, in case of an interactive proof of, say \verb+! x \ P ' x+, after applying the introduction rule of forall the new sequent becomes \verb+ |- P ' a+ where the variable \verb+a+ is now bound by a meta-level \verb+pi a \ + that was traversed during the proof search. Unfortunately and for efficiency reasons, LambdaProlog does not remember the names of bound variables when execution goes under a \verb+pi+, but only their De Brujin level~\cite{???}. Therefore in the goal presented to the user the name \verb+a+ will be automatically generated without any control and no correlation to the initial \verb+x+.

The problem can be improved at least in two ways: 1) modifying ELPI to remember names, at the price of performance; 2) making the hypothetical pre-processor discussed above insert string parameters in the syntax so that \verb+! x \ P+ would be parsed as \verb+! "x" x \ P+ and later processed via \verb+do_something (! s F) :- pi  x \ parse s x => do_something (F x).+

\subsection{Top-level loop}

HOL Light does not have an ad-hoc interactive loop: the OCaml top-level is used instead thanks to the private data types. In our implementation, instead, the toplevel loop mechanism is also part of the kernel and, as for the tactic loop, it asks a \emph{library certificate} (\verb+WHAT+ in the code) the next command to be executed
(\verb+C+) and the continuation certificate (\verb+CONT+):

\begin{verbatim}
check WHAT :-
 next_object WHAT C CONT,
 (C = stop, !, K = true ; check1 C H , check_hyps [] H, K = (H => check CONT)),
 !, K.
\end{verbatim}

If \verb+C+ is \verb+stop+ the toplevel loop ends successfully. Otherwise the command \verb+C+ is executed (by \verb+check1+) resulting in a new hypothesis \verb+H+ that is verified (by \verb+check_hyps+) and then assumed in the next iteration of the loop (\verb+H => check CONT+). The only available commands in HOL are: assuming a new axiom; declaring a new (polymorphic) definition; defining a new type as a conservative extension; proving a theorem. HOL packages can be easily implemented by the \verb+next_object+ predicate that is defined outside the kernel. For example, we implemented an \verb+inductive_def+ package that takes a piece of syntax that declares the introduction rules of a new inductively defined predicate and returns a bunch of definitions and theorems based on the Knaster-Tarski's fixpoint theorem. There are only a few kind of hypotheses returned by \verb+check1+ (that is defined in the kernel): assignment of a type to a constant (i.e. \verb+term c ty+), declaration of a constant as a type constructor (e.g. \verb+typ (list '' A) :- typ A.+), assigning a statement to a theorem/axiom name (i.e. \verb+provable c statement+). The \verb+check_hyps+ predicate just verifies that constants are assigned a type/statement once.

As for tactics, we developed two kind of library certificates: the first one is non interactive and it consists in just a list of commands to be expanded by packages in the clauses of \verb+next_object+; the second one is interactive and it just parses from standard input the next command to be executed. In the case the command starts a proof, the interactive library certificate will use an interactive certificate for the proof, effectively entering the proof mode of the theorem prover.

Finally, backtracking inside a proof is as simple as entering a \verb+backtrack+ command that just fails, invoking LambdaProlog backtracking mechanism. Note, however, that the behaviour is different than the one of HOL Light: backtracking can enumerate the next solution of a previous tactic, unless a Prolog cut was used to prune the remaining solutions.

\subsection{Definitional Mechanisms}

We implemented definitional mechanisms similar to the one of HOL Light, but modified to have a constructive interpretation (see~\cite{??} for an explanation of why the ones of HOL Light entail excluded middle). The only significant change is about polymorphic constants where we preferred to employ System-F application, written in infix style using two quotes, e.g. \verb+monotone '' nat ' f+. Syntactically, the definition of monotone is as follows:

\begin{verbatim}
def monotone (pi A \ (((A --> prop) --> (A --> prop)) --> prop),
  (lam (_ A) f \ ! x \ ! y \ x <<= y ==> f ' x <<= f ' y))
\end{verbatim}

where the type abstraction binds \verb+A+ both in the type and in the body.
The new hypothesis that is added to the kernel after processing the definition
is \verb+pi A \ term ' (monotone '' A) ' (((A --> prop) --> (A --> prop)) --> prop)+. When LambdaProlog backchains over the hypothesis, it automatically instantiates \verb+A+ with a fresh metavariable. Therefore we get the expected behaviour for free, without the need to syntactically introduce System-F type abstractions and type quantifiers.

Despite the System-F terminology we employ, note that, as expected for an HOL system, the polymorphism achieved is the Hindley-Milner one, where quantifiers do not occur inside types.

\section{Conclusions and Future Work}

%\appendix
%\section{Appendix Title}

%This is the text of the appendix, if you need one.

\acks

We are greatly indebted with Dale Miller for long discussions over LambdaProlog, its use for implementing interactive provers and the constraint programming extensions.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}


\end{document}
