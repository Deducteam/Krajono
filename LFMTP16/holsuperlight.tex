%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% numbers       To obtain numeric citation style instead of author/year.

\usepackage{amsmath}
\usepackage{fancyvrb}
\usepackage{bussproofs}

\newcommand{\cL}{{\cal L}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{LFMTP '16}{June 23, 2016, Porto, Portugal}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\copyrightdoi{nnnnnnn.nnnnnnn}

% Uncomment the publication rights you want to use.
%\publicationrights{transferred}
%\publicationrights{licensed}     % this is the default
%\publicationrights{author-pays}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{A lightweight implementation of an HOL system in an extension of higher order logic programming}   % 'preprint' option specified.

\title{Implementing HOL in an Higher Order Logic Programming Language}
%\subtitle{Subtitle Text, if any}

\authorinfo{Cvetan Dunchev \and Claudio Sacerdoti Coen}
           {University of Bologna}
           {cdunchev@hotmail.com/claudio.sacerdoticoen@unibo.it}
\authorinfo{Enrico Tassi}
           {INRIA Sophia-Antipolis}
           {enrico.tassi@inria.fr}

\maketitle

\begin{abstract}
We present a proof-of-concept prototype of a (constructive variant of an) HOL interactive theorem prover written in a Higher Order Logic Programming (HOLP) language, namely an extension of LambdaProlog. The prototype is meant to support the claim, that we reinforce, that HOLP is the class of languages that provides the right abstraction level and programming primitives to obtain concise implementations of theorem provers. We identify and advocate for a programming technique, that we call semi-shallow embedding, while at the same time identifying the reasons why pure LambdaProlog is not sufficient to support that technique, and it needs to be extended.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%\terms
%term1, term2

\keywords
LambdaProlog, HOL, Higher Order Logic Programming, Constraints

\section{Introduction}
What are the programming paradigm and the programming languages better suited for the implementation of interactive theorem provers? Better suited here means providing high level primitives at the good level of abstraction, relieving the programmer from the burden of (re)implementing the basic mechanisms a theorem prover relies on.

Every (interactive) theorem prover:
\begin{enumerate}
\item Has to manipulate expressions with binders up to $\alpha$-conversion, and it needs to implement substitution. Binders are ubiqutous: they occur in formulae, but also in both procedural and declarative scripts (new hypotheses are named and have an associated scope), and in proof objects as well.
\item Has to implement automatic proof search, either in the small or in the large, that requires to explore the search space via backtracking. Because the search space can be very large, the programmer needs way to control backtracking and direct the exploration.
\item Has to manipulate incomplete data structures, i.e. data structures having parts not specified yet. Moreover, invariants have to be enforced on the data structures and lazily checked when the data structures get instantiated. Examples of incomplete data occurs in formulae (omission of types to be inferred), sequents (omission of terms to be identified later, e.g. writing $X$ for a yet unknown witness of an existential statement), and proof objects (an incomplete proof has a proof object containing an hole).
\end{enumerate}

The three features discussed above also interact in complex ways. For example, renaming bound variables in a term that contains metavariables (i.e. omitted sub-terms) must record the renaming as an explicit substitution applied to the metavariable to resume the renaming when the metavariable will be istantiated. Or, backtracking must be aware of dependencies between subgoals due to sharing of metavariables in order to split the subgoals into independent clusters of goals such that once a cluster is solved it becomes useless to backtrack that solution because it won't affect the search space of the other goals.

In a series of papers from the XXs \cite{????}, Amy Felty already advocated Higher Order Logic Programming (HOLP) as the programming paradigm best suited for the three tasks above. LambdaProlog, the flagship of HOLP languages, manipulates syntax with binders, relieving the programmer from problems due to renaming and substitution; being a logic language it has backtracking for free; it allows to control backtracking via the usual Prolog's cut ("!") whose semantics and pragmatics is well known, even if it does not fit completely well the logical reading of the language; finally the language uses metavariables that range over both data and functions, and the complex implementation of LambdaProlog takes care of all interactions between binders ($\lambda$-abstraction) and metavariables, in particular addressing the problem of higher-order unification under mixed prefixes~\cite{??}.

Despite the push by Amy Felty, none of the interactive theorem provers in use is implemented in HOLP. With the exception of Mizar, implemented in Pascal for historical reasons, and the new Lean system, implemented in C++ for efficiency, all other systems (Coq, Isabelle, Agda, Matita, etc. \cite{?17thproversoftheworld??}) are implemented in either Haskell or an ML variant.

In general, functional languages provide a good compromise between performance and high-level coding, and algebraic data types can naturally encode syntax without binders. In particular, ML was born as the language for implementing the LCF theorem prover, one of the first theorem provers of the world. Nevertheless, these languages solve none of the three problems above:
\begin{enumerate}
\item with the exception of FreshML, that has also not been used for interactive theorem proving yet, functional languages do not abstract the details about binders and the user must encode bound variables via De Brujin indexes (like in Coq and Matita) or it must implement $\alpha$-conversion and substitution carefully (like in HOL Light). De Brujin indexes allow the code to be very efficient, but code that handles them is very error prone.
\item the ML and Haskell families do not provide backtracking for free. A limited form of backtracking can be implemented in ML via exceptions: an exception can be raised to backtrack to a previous state, but once a function call is over it is not possible to backtrack into it any longer. Continuations based monads to implement backtracking (plus state to handle metavariables and failure) exists~\cite{?????}, but the code is quite complicated and it has efficiency problems unless it is manually optimized (Section~X of~\cite{????}).
\item managing metavariables, their interaction with binders and higher order unification of fragments of it requires a lot of fragile and complex code (e.g. $\approx$ 3100 lines of OCaml code for Matita, compared to the $\approx$ 1500 lines for the kernel that implements $\beta$-reduction, conversion and the inference rules of the logic).
\end{enumerate}

The situation described above is bad in two respects.
\begin{enumerate}
\item All the code that deals with the three problems is essentially logic-independent, but it requires to be re-implemented when ones wants to experiment with a new logic or implement a new system. With the exception of the monad cited above, it also seems hard to encapsulate the boilerplate code into a reusable library: a real extension of the programming language is necessary.
\item The code of the systems becomes very low-level, having to deal with all kind of intricacies due to binders and metavariables. Therefore it is hard for external programmers to contribute to the code base, for example to implement new domain specific tactics. The result is that these systems often implement in user space a second programming language, exposed to the user to write tactics, that takes care of binding, metavariables, backtracking and its control. For example, LTac~\cite{???} is such a programming language for Coq, that also sports several other mini-languages to let the user customize the behaviour of the system (e.g. to declare type classes~\cite{???}, used to provide unification hints~\cite{???}). Not only the system becomes more complex because of the need to provide and interpret a new programming language on top, but its semantics is problematic: the beaviour of the system becomes the combination of pieces of code written in multiple languages and interacting in non trivial ways. Static analysis of this kind of code is out of reach.
\end{enumerate}

Adopting an HOLP language seems a promising idea. First of all the boilerplate code is pushed once and for all in the interpreter of the language, becoming decoupled from the programming logic of the theorem prover. Therefore the code of the theorem prover becomes much more clean, and very close to the inference rules it implements. Secondly, because of the decoupling, it becomes possible to experiment with alternative implementations of the HOLP language, e.g. susbtituting De Brujin levels for the indexes (like in~\cite{???}) or profiling the implementation of the language on different applications. Thirdly, and most importantly, there is no more any need for ad-hoc languages in user space: the users that want can directly contribute to the code base of the theorem proving ignoring all the gory details.

Nevertheless, as we said, no system has been implemented in LambdaProlog, despite the interest raised by the work of Felty (her XXX paper~\cite{???} has more than YYY citations). One reason is due to performance: logic languages are hard to optimize, and LambdaProlog is orders of magnitudes harder than Prolog not only because of binders and higher order unification, but also because it is higher order (one can pass predicates around) and it allows a primitive (logical implication \texttt{P => Q}) to temporarily augment the code of the program (with \texttt{P}, while proving \texttt{Q}) in a lexically scoped way, making some well known Prolog static analyses and optimizations hard. Moreover, LambdaProlog had for some time only very slow implementations, until Teyjus was born~\cite{???} after the work of Felty. Recent work by the authors also showed that the performances of Teyjus are not so good~\cite{???}, considering that Teyjus compiles the code to an extension of the Warren abstract machine instruction set, while the ELPI interpreter of the authors that attempts no compilation is able to run consistently faster than Teyjus (Section XXX of~\cite{???}).

In this paper we push seriously the idea of implementing in the ELPI variant of LambdaProlog an interactive theorem prover, inspired by HOL Light, but for a constructive variant of the logic. The aim of the experiment is to come up with implementation guidelines, that differ from the ones of Amy Felty, and to quantify what is the loss in term of performance w.r.t. systems implemented in functional languages.

We also note that our expertise acquired with the implementation of Matita was put to full already in the implementation of our ELPI interpreter. Indeed, we essentially had to split out the code of Matita that deals with the three problems above, and figure out how to integrate it in an interpreter for a logic language. In the long term, we would like to scale the HOL experiment to a new implementation of Matita/Coq (or at least its logical core, that excludes the user interface).

Interestingly enough, the methodology we identified, presented in Section~\ref{UUU}, requires significant extensions of LambdaProlog, that we partially implemented in ELPI and that we briefly describe here as well.

As a final remark, the interest of the work, that is still on-going, is not to obtain a new competitive implementation of HOL, that would require several men years and could be of limited interest. In particular, we only developed the system so far to the point where all the parts of a typical theorem prover implementation for HOL are represented to judge the feasibility and economy of the implementation.

Section~\ref{????} \ldots

\section{LambdaProlog in a nutshell}
LambdaProlog extends the core of Prolog in two directions.
\begin{enumerate}
\item it allows in the syntax $\lambda$-abstractions (written \verb+x\p+ for $\lambda x.p$), applications (written \verb+p q+ like in $\lambda$-calculus and unlike in Prolog) and metavariables both in argument and in head position (e.g. \verb+iter [X|XS] F [Y|YS] :- F X Y, iter XS F YS.+ where the metavariable \verb+F+ is to be istantiated with a binary predicate like \verb,(x\y\y=x+x),)
\item it generalizes Horn clauses to Higher Order Hereditary Harrop Formulae (HOHHF) clauses, whose grammar is
$$\begin{array}{l}
H ::= x~t_1~\ldots~t_n ~|~ H \wedge H ~|~ \forall X.H ~|~ \exists x.H ~|~ G \Rightarrow H\\
G ::= x~t_1~\ldots~t_n ~|~ X~t_1~\ldots~t_n ~|~ G \wedge G ~|~ G \vee G\\
 \hspace{0.65cm} ~|~ \forall x.G ~|~ \exists X.G ~|~ H \Rightarrow G\\
H,G \subseteq t ::= x ~|~ X ~|~ tt ~|~ x\backslash t
\end{array}$$
where $H$ ranges over HOHHF, $G$ ranges over goal formulae, $x$ are constants/universally quantified variables, $X$ are existentially quantified variables and $t$ are the higher order terms of the language. A LambdaProlog program is a list of HOHHF formulae. To run a LambdaProlog program the user submits a query $G$ that is automatically proved using the clauses in the program.
\end{enumerate}

The (operational) semantics of the connectives that occurs respectively in goal formulae $G$ / in HOHHF $H$ is given by the introduction/elimination rules of the connectives in natural deduction for intuitionistic higher order logic. In particular:
\begin{itemize}
\item the goal $H_1 \wedge H_2$ is turned into the two goals $H_1$ and $H_2$,
 copying the program (i.e. the set of clauses/assumptions)
\item the goal $H_1 \vee H_2$ is turned into the goal $H_1$; in case of failure of the proof search, one can backtrack and restart with the goal $H_2$
\item $H \Rightarrow G$ assumes $H$, that is turned into a new program clause, to prove $G$
\item $\forall x.G$ introduces a fresh variable $y$ and proves $G$ after replacing $x$ with $y$ in $G$
\item $\exists X.G$ introduces a fresh metavariable $Y$ and proves $G$ after replacing $X$ with $Y$ in $G$. Later $X$ can be istantiated with any term whose free variables were all in scope at the time $\exists X.G$ was processed
\item the goal $t$ where $t$ is atomic is proved unifying $t$ with the
(hereditary) conclusion of one of the program clauses, possibly opening new
goals as well (see case $G \Rightarrow H$ below)
\item assuming $H_1 \wedge H_2$ means assuming both $H$s independently
\item assuming $\forall X.H$ means assuming an infinite number of copies of $H$ for all fresh metavariables $Y$ substituted for $X$ in $H$. Concretely, the copy and substitution is performed lazily when the clause is used
\item assuming $\exists x.H$ means generating a new fresh constant $y$ and assuming $H$ after replacing $x$ with $y$
\item assuming $G \Rightarrow H$ means assuming that $H$ holds under the assumption that $G$ holds as well. When the clause is used, the new goal $G$ will be added to the set of open goals to be proved
\end{itemize}

In concrete syntax we will write \verb+sigma X\t+ for $\exists X.t$, \verb+pi x\t+ for $\forall x.t$, \verb+t_1 => t_2+ or \verb+t_2 := t_1+ for $t_1 \Rightarrow t_2$, \verb+t1,t2+ for $t_1 \wedge t_2$ and \verb+t1;t2+ for $t_1 \vee t_2$. Moreover, all free uppercase/lowercase variables in a program clause are implicitly universally/existentially quantified globally/locally (like in Prolog).

\texttt{sigma} in clause positions are not in ``standard'' LambdaProlog and they are accepted by our ELPI interpreter, but not from Teyjus. However, Teyjus implements a module system~\cite{???} that allows to declare variables local to the list $H_1,\ldots, H_n$ of clauses of the module. The same behaviour can be obtained in ELPI with (\texttt{sigma x \ }$H_1, \ldots, H_n$) or with the equivalent
syntactic sugar \texttt{\{ local x.} $H_1.\;\ldots\; H_n.$ \texttt{\}} (where the curly braces are used to delimit the scope of the existential quantifier, i.e. the local declaration). Declaring local constants is the only and fundamental mechanism in LambdaProlog to restrict the trusted code base.

The other major difference between the ELPI version of LambdaProlog and the ``official'' one of Teyjus is that we do not enforce any type discipline, nor we try to statically enforce the restriction to HOHHF. Errors at run-time (e.g. invoking a list as a predicate) result in abortion of the program execution. On the other hand, it is for example possible to read from the terminal a query and execute it, which is prohibited by Teyjus because the correctness of the query cannot be statically enforced.

To illustrate a typical LambdaProlog example, in Table~\ref{type-checker1} we show the code of a type-checker for closed terms of the simply typed lambda-calculus. We use the infix constant \verb+'+ to encode application, the unary constant \verb+lam+ for lambda-abstraction, and the infix constant \verb+-->+ for the function space type. The latter is to be applied to a meta-level lamdba-abstraction in the spirit of higher-order abstract syntax. For example, the term $\lambda x.xx$ is encoded as \verb+lam x \ x ' x+. Note that, in the concrete syntax, parentheses around lambda-abstractions are not necessary in LambdaProlog when the abstraction is the last argument of an application. I.e. \verb+lam x \ x ' x+ is to be read as \verb+lam (x \ x ' x)+.

Observe that the code of the type-checker is really minimal and in one-to-one correspondence with the two derivation rules one writes usually on paper. However, there are some differences as well. In particular, the typing judgement \texttt{term T TY} (``\texttt{T}'' is a term of type ``\texttt{TY}'') does not mention any context $\Gamma$. Instead, in the rule for lambda-abstraction, the hypothesis that the fresh variable \texttt{x} has type \texttt{A} is directly assumed and put temporarily in the program code using logical implication \texttt{=>}.
For example, the query \texttt{term (lam y \ y ' y) TY} will match the second
clause and trigger the new query \texttt{term (x ' x) B} after having istantiated \texttt{TY} with \texttt{A --> B} where \texttt{x} is fresh (because introduced by \texttt{pi}) and it is known to have type \texttt{A}.

Finally, we will use the cut predicate of Prolog \texttt{!} with the same
non-logical semantics. We say that a predicate does not have a logical semantics
when it breaks the commutativity rule of conunction or, equivalently, commutation of the semantics with istantiation.

\begin{table}[t]
\begin{small}
\begin{Verbatim}[numbers=left,numbersep=1pt,frame=leftline]
term (M ' N) B :- term M (A --> B), term N A.
term (lam F) (A --> B) :- pi x\ term x A => term (F x) B.
\end{Verbatim}
\end{small}
\caption{\label{type-checker1}A type-checker for simply typed lambda calculus.}
\end{table}

\section{HOL in a nutshell and its implementation in ML}
HOL is a variant of Church's simple theory of types. Its precise syntax and semantics can be found in~\cite{the_HOL_system_LOGIC} even if we will implement a constructive variant of it, and we will refer for inspiration to the HOL Light version and implementation of a classical variant of HOL. Types and terms have the
following syntax:
$$\begin{array}{l}
T ::= \alpha ~|~ u~T_1\ldots T_n ~|~ T \rightarrow T\\
t ::= x_T ~|~ c_T ~|~ \lambda x_T. t ~|~ tt
\end{array}$$
where $u$ ranges over type constructors (constants when $n=0$) and $\alpha$
ranges over type variables. Contrarily to System-F~\cite{???}, HOL has no type
abstractions/applications. However, it is possible to assign to constants
schematic types containing type variables $\alpha, \beta, \ldots$. For example,
the append function \texttt{@} can be typed as $\mathtt{list}~\alpha \rightarrow \mathtt{list}~\alpha \rightarrow \mathtt{list}~\alpha$.

Variables $x_T$ and constants $c_T$ carry explicitly their type $T$ and they are equal iff both the name and the types are. In particular, each occurrence of a polymorphic constant, carrying its type, explicitly carries an instantiation for every type variable occurring in its type. For example
\texttt{@}$_{\mathtt{list}~\mathtt{nat} \rightarrow \mathtt{list}~\mathtt{nat} \rightarrow \mathtt{list}~\mathtt{nat}}$ implicitly encodes the instantiation $\alpha := \mathtt{nat}$. In System-F the constant occurrence would be applied to the type: $\texttt{@}~\mathtt{nat}$.

HOL systems assume the existence of two predefined types, one for propositions \texttt{bool} and one for individuals \texttt{ind} (also written $o$ and $\iota$ in the litterature). Logical connectives and quantifiers are just single out constants. For example, conjuction is given the monomorphic type $\mathtt{bool} \rightarrow \mathtt{bool} \rightarrow \mathtt{bool}$ and universal quantification the polymorphic type scheme $(\alpha \rightarrow \mathtt{bool}) \rightarrow \mathtt{bool}$. The usual trick of higher-order syntax is used to reduce all binders to $\lambda$-abstraction: $\forall x:\mathtt{nat}.P$ is represented as $\mathtt{forall}_{(\mathtt{nat} \rightarrow \mathtt{bool}) \rightarrow \mathtt{bool}} (\lambda x.P)$. In the case of the HOL Light variant, the only constant that is initially assumed to exist is polymorphic equality $\mathtt{=}_{\alpha \rightarrow \alpha \rightarrow \mathtt{bool}}$, that also doubles for coimplication when $\alpha = \mathtt{bool}$.

The derivation system of the logic is made of two judgements, one for typing terms and the other for proving formulae in contexts (see ~\cite{???} for the XX derivation rules of the HOL Light implementation). Two examples of such rules are

\AxiomC{$\vdash p : \mathtt{bool}$}
\AxiomC{$\Gamma_1,p \vdash q$}
\AxiomC{$\Gamma_2,q \vdash p$}
\RightLabel{\tiny DEDUCT\_ANTISYM\_RULE \label{krule}}
\TrinaryInfC{$\Gamma_1,\Gamma2 \vdash p = q$}
\DisplayProof

\AxiomC{$\Gamma \vdash p$}
\RightLabel{\tiny INST\_RULE}
\UnaryInfC{$\Gamma \sigma \vdash p \sigma$}
\DisplayProof

Like in the HOL Light implementation, the check for $p$ being a boolean in the former rule can be post-poned to the time the hypothesis is used (the HYP rule, here not shown).
The latter rule is used to apply the substitution $\sigma$ that maps free variable names to terms. Its application must avoid variable capturing.

An HOL theory is a list of declarations of types, constants and axioms, the latter being just formulae assumed to hold.

The implementation of an HOL systems is rooted in the principle of conservative extensions: in order to introduce a new constant or a new type that satisfy certain axioms, the user must prove the existence of such a constant/type. We only show here the rule for introducing new definitions to explain a kind of complexity that involves free and bound names. 

To introduce a new (polymorphic) constant $c_T$, the user must give a term $t$
of type $T$ such that all free type variables contained in $t$ also occur in
$T$. After checking the conditions and veryfing that no constant named $c$ is already defined in the current theory, the latter is extended with the new
constant $c_T$ and the axiom $c_T = t$.

The condition on the free type variables of $t$ avoids logical inconsistency
and would not be necessary in System-F where every occurrence of $c$ would be explicitly applied to every free type variable that occurs in $t$. In HOL, instead, only the type instantiations for the type variables that occur in the type of $c$ can be recovered from the type of $c$. Therefore, if the condition is violated, $c_T$ could be rewritten via the axiom to an instantiation of $t$ that still contains free type variables. Example: $c_\mathtt{bool} = ! x_\alpha y_\alpha \  x_\alpha = y_\alpha$ that from $c_\mathtt{bool} = c_\mathtt{bool}$ via two instantiations would imply $! x_\mathtt{bool} y_\mathtt{bool} \  x_\mathtt{bool}= y_\mathtt{bool} \iff ! x_\mathtt{unit} y_\mathtt{unit} \  x_\mathtt{unit}= y_\mathtt{unit}$ i.e. $\mathtt{false} = \mathtt{true}$.

\subsection{HOL (Light) in ML}
HOL Light~\cite{??} is an extremely lightweight implementation of HOL in ML that reuses the ML toplevel, following the tradition started by Milner~\cite{???}. The key idea is the possibility in ML of encoding \emph{private data types}~\cite{??}, i.e. data types whose values can be inspected via pattern matching outside the module that defines them, but that cannot be directly constructed outside the module. Instead, from outside the module, it is only possible to invoke a function, implemented in the module, that returns an inhabitant of the type.

Private types can be used to encorce invariants: the functions of the module only builds terms that satisfy the invariants. Therefore, all inhabitants built outside the module are correct by construction, and the trusted code base for the maintainance of the invariants is lexically delimited by the module.

HOL Light implements two private types corresponding to the two judgement of the logic: 1) the type of well typed terms $t$ such that there exists a $T$ s.t. $\vdash t : T$; 2) the type of theorems, i.e. pairs context $\Gamma$, formula $F$ such that $\Gamma \vdash F$. Theories could also have been implemented as a third private type (like what Coq does), but they are actually hidden inside the module that define the type of (well typed) terms.

In particular, every inference rule of HOL corresponds to a function over theorems exported from the ``kernel'' module of HOL.

\noindent
\verb+val mkComb : term -> term -> term+\\
\verb+val DEDUCT\_ANTISYM\_RULE : thm -> thm -> thm+\\
for example correspond respectively to the typing rule for application $t_1 t_2$ and to the inference rule shown on page~\ref{krule}. The function\\
\verb+val dest_thm : thm -> term list * term+\\
allows to inspect a theorem to recover what has been proved. Of course, it does not have an inverse function that would correspond to claiming that $\Gamma \vdash p$ is a theorem without actually proving it.

An important feature of this approach, that dates back to LCF and puts it apart from the Curry-Howard approach used for Coq and other systems for dependently typed lanugages, is that \emph{no proof object} need to be stored. Indeed, an inhabitant of \verb+thm+ really is (in HOL Light) just a pair hypotheses/conclusion.

The functions implemented in the kernel already allow to build proofs in bottom-up style. Quite frequently, however, the user would like to mix the bottom-up and the top-down styles, and he would like to be able to use metavariables for yet unknown formulae during a top-down proof.

Again following Milner's LCF, HOL Light implements outside the kernel a mechanism for top-down proofs based on the notion of tactic. Simplifying, the tactic type is defined as\\
\verb+type tactic =+\\
\verb+  sequent -> sequent list * instantiation * metavariables * justification+\\
\verb+type justification = instantiation -> thm list -> thm+

Ignoring the instantiation and metavariables, a tactic is a function that takes in input the sequent to be proved and return a list of new sequents to be proved and a justification. When the new sequents are proved, a list of theorems is obtained. The justification, fed with this list of theorems, returns the theorem for the initial sequent. In other words, each tactic implements a top-down reasoning step by producing the list of new subgoals plus the ``inverse'' bottom-up proof. When those are composed, the theorem is proved. Variables recorded as metavariables, i.e. omitted information, and instantiations for them complete the picture.

We make a few observations here:
\begin{itemize}
\item As we already noticed in~\cite{??}, LCF tactics have a few drawbacks. They only allow to reason locally on one goal, because they do not take in input the global proof state. In case of mutual dependencies between goals due to metavariables, it becomes impossible to reduce the search space using global reasoning. Moreover, it is not possible to interrupt the execution of a tactic in the middle, for example to observe the intermediate states. The tinycals introduced in~\cite{??}, which can now be implemented in Coq as well after~\cite{??}, are a better solution but they require a first class representation of the entire proof state.
\item During a top-down proof, the system temporarily records a proof object in the form of a justification, that is lazily consumed as soon as the corresponding subproof is completed (all leaves are closed). We would like to avoid this if possible.
\item Every bottom-up inference rule --- primitive or derived --- performs some checks that are performed twice when the rule is turned into a top-down tactic.
For example, the top-down tactic for the \verb+DEDUCT\_ANTISYM\_RULE+ rule must analyse the sequent in input to verify that it is of the form $\Gamma \vdash p=q$, yielding the two new goals $\Gamma, q \vdash p$ and $\Gamma, p \vdash q$. Later the justification \verb+DEDUCT\_ANTISYM\_RULE+ needs to pattern match the list of theorems in input against $\Gamma_1,q \vdash p$ and $\Gamma_2,p \vdash q$. The ``duplicate'' inverse checks are necessary because the kernel does not trust the outer parts of the system, but are still somehow undesired.
\end{itemize}

\section{Requirements and Term Encoding}
We begin the description of our implementation of (a constructive variant of)
HOL in ELPI, our LambdaProlog interpreter. The code described here is available at~XXXXX. The implementation is meant as an experiment to evaluate if Higher Order Logic Programming can indeed be employed concisely and effectively for the implementation of interactive theorem provers. In this experiment, we take inspiration from HOL Light and we set the following requirements:
\begin{enumerate}
\item The system must have a small trusted code based, henceforth called the ``kernel''. Bugs outside the kernel must not allow to conclude that a non-tautological formula $F$ is a theorem.
\item The LCF architecture does not need to record proof objects. Therefore we do not want to record proof objects as well.
\item The implementation must use genuine Higher Order Logic Programming techniques. Binders have to be handled exploiting the binding machinery of the language. Object level metavariables must be represented by the metavariables of the language.
\item Tactics must be able to inspect the global proof state.
\end{enumerate}

The third requirement rules out the encoding of the HOL Light in LambdaProlog. A mechanical encoding is possible because the language is Turing complete, but it would not benefit from the features of the language.

The second and fourth requirement rule out the technique used by Amy Felty and, more recently, by the group of Dale Miller~\cite{???}. Felty and Miller encodes the judgements of the object logic via a LambdaProlog predicate that defined by recursion over a proof object (Felty) or a proof certificate (Miller).
For example, the proof object can decide to apply the transitivity rule for application to a goal where multiple rules apply. A proof certificate is similar to a proof object, but it can avoid recording all the needed information or encode it at a different level of detail. Therefore, a proof certificate may be non syntax directed, in the sense that multiple rules can be tried according to the same certificate, triggering backtracking if needed.

In both cases, the system can be simply turned into both an automatic or an interactive theorem prover. In the first case, it is sufficient to replace the certificate with a metavariable: the LambdaProlog predicate will then work in a generative way, blindly enumerating all possible proof objects (and potentially diverging because of depth-first search, that can be replaced with iterative deepening to keep completeness). To obtain an interactive prover, the certificate is replaced with the request to the user to type the next rule to be tried. The user interaction can also print the sequent to be proved, but not the other open branches of the proof, that are kept at the meta-level in the and/or tree of the LambdaProlog interpreter.

Finally, we cannot implement the kernel using a private type like in ML because Logic Programming rules out private types. To understand why, remember that every predicate in a (pure) logic programming language is invertible. Therefore it is not possible to implement a type with a predicate to reveal the data, but preventing its use to build the data as well. In particular, the predicate \verb+dest\_thm+ corresponding to the function with the same name in HOL would allow to feed it any pair context-formula to obtain a theorem without any further control.

The only way to protect part of the code in LamdaProlog is to existentially quantify a predicate to be kept secret to the rest of the code. Therefore, our kernel implements two predicates, one for the typing judgement and one for derivability, that are kept local. In order to fullfill requirement number 4, derivability will work on list of sequents (the proof state) in place of a single sequent. Finally, to avoid the repetition of code and checks in the HOL approach, we only focus for now on top-down proofs.

We detail now the implementation of the kernel.

\subsection{Semi-shallow encoding of terms}
Before starting, we first need to choose a syntactic representation for both
terms and types that respect requirement 3, that rules out deep encodings.
A deep encoding for simply typed $\lambda$-calculus with metavariables would
look like this and be very similar to the data type Coq and Matita are build around:

$$ t ::= var~\mathtt{NAME} ~|~ app~t~t ~|~ lam~\mathtt{TYPE}~\mathtt{NAME}~t ~|~ meta~\mathtt{NAME}$$

Assuming $\mathtt{NAME}$s to be either strings or De Brujin indexes, a deep encoding forces the implmenetation of $\alpha$-conversion, lifting, substitution, instantiation of metavariables, etc.

At the other side of the spectrum we find the shallow encoding for closed terms: we just encode object level terms with language level terms, object level metavariables with LambdaProlog metavariables and so on. Instantiation of metavariables as well as $\alpha$-conversion and $\beta$-reduction comes for free. However, it becomes impossible to implement --- without extra-logical extensions of the language --- both the derivation and the typing judgements. For example, the code
\verb+type (F X) B :- type X A, type F (A --> B)+ does not do what it seems to
do, because applied to the query \verb+type (f a) Z+, it yields the higher order
unification problem \verb+F X = f a+ that admits multiple solutions like
\verb+F := x \ f a+, \verb+X := 0+.

We therefore propose what we call the \emph{semi-shallow} encoding:

$$t ::= t~\verb+'+~t ~|~ lam~\mathtt{TYPE}~F$$

where \verb+'+ is just an infix constructor, \verb+F+ is a LambdaProlog function from terms to terms and object level metavariables are encoded via language level metavariables. For example, $\lambda x\!:\!o. X~x$ is encoded as
\verb+lam o x \ X x+. Ignoring the issue about metavariables, the semi-shallow encoding was the one already used in Table~\ref{type-checker1}, that essentially show the typing judgement of our implementation of HOL Light, which includes a few other clauses for primitive constants, like \verb+term eq (A --> A --> prop)+.

For simply typed $\lambda$-calculus (plus type variables) type checking is decidable and the typing judgement of Table~\ref{type-checker1} is already syntax directed. Therefore HOL never asks the user to prove that a term has a certain type: type checking/inference is run fully automatically.

Because we can inject LambdaProlog variables in place of types of bound variables, our type checking judgement already doubles as type inference. E.g. \verb+term (lam A x \ x + 1) B+ yields \verb+A := nat, B := nat+.

\section{Towards a Constrained Programming Extension of LambdaProlog}
What happens when the typing judgement is called on a term that contains a metavariables (like in \verb+lam A x \ X x+)? Without changes to the system, the \verb+term+ predicate will start blindly enumerating all well typed terms in generative mode. This is definitely not the behaviour we expect from an interactive prover.

The correct behaviour, implemented for example by Coq as well, is to stop execution of the type-checking judgement, to be resumed only when the metavariable will be istantiated. A more insightful way to read this behaviour is to imagine that the judgement is turned into a \emph{constraint} imposed on the metavariable, that ranges over the infinite domain of terms. In particular, \verb+term X T+ is turned into the constraint that \verb+X+ must have type \verb+T+. When later \verb+X+ is instantiated, the constraint is verified.

The explanation based on constraints is particularly insightful because it suggests the other fundamental operation of Constraint Programming: constraint propagation. Imagine, for example, that the same metavariable \verb+X+ occurs twice in a term to be type-checked. Type-checking will impose two constraints over \verb+X+, for example \verb+term X (A --> nat)+ and \verb+term X B+. Because of unicity of typing, a metatheoretical result about the simply typed $\lambda$-calculus, we know that this can be the case iff \verb+(A --> nat) = B+. Therefore the metatheorem can be turned into a contraint propagation rule that, given two typing constraints on the same variable, removes one of them generating a new unification problem.

Propagation of typing constraints is implemented in Coq in an ad-hoc way. The Lean implementation, instead, starts building a generic constraint programming machinery on top of C++~\cite{???}. Similarly, we want to extend our LambdaProlog langauge to an \emph{Higher Order Constraint Logic Programming Language} where the set of constraints and propagation rules are described in user space, much in the spirit of the Constraint Handling Rules (CHR) language.

Work in this direction is in progress. As a first step that does not involve constraint propagation, we would like to specify that a syntax directed predicate is to be turned into a constraint when it faces a metavariable. Hypothetical syntax: \verb+delay (term X T) until not flexible X.+ For the time being, we encode the syntax above with two low-level primitives not in standard LambdaProlog:
\begin{enumerate}
\item \verb+$is_flex X+ holds iff \verb+X+ is a flexible term, i.e. a metavariable or an application of a metavariable.
\item \verb+$delay G X+ always holds, putting the goal \verb+G+ into a list of delayed goals, to be resumed as soon as the metavariable \verb+X+ is instantiated.
\end{enumerate}

With these two primitives, turning \verb+term+ into a constraint on flexible terms can be encoded as in Table~\ref{type-checker2}.
\begin{table}[t]
\begin{small}
\begin{verbatim}
term T TY :- $is_flex T, !, $delay (term T TY) T.
term T TY :- term' T TY.
term' (lam A F) (A --> B) :- pi x\ term' x A => term (F x) B.
term' (F ' T) B :- term F (A --> B), term T A.
term' eq (A --> A --> prop).
\end{verbatim}
\end{small}
\caption{\label{type-checker2}A type-checker for simply typed lambda calculus in Constraint Programming style.}
\end{table}

The first line delays the goal \verb+term T TY+ iff \verb+T+ is flexible. Otherwise the second line immediately turns it into \verb+term' T TY+. The next lines are the old implementation of \verb+term+ where \verb+term'+ is used in place of \verb+term+ in the heads of the clauses. The reason for the introduction of \verb+term'+ is that the new clauses \verb+term' x A+ introduced in the body of the lambda-abstraction case must be used strictly after the first, delaying clause. According to the standard semantics of LamdaProlog, a clause introduced by logical implication is always tried first. Hence the need for the technical \verb+term/term'+ distinction.

The \verb+$is_flex+ predicate is clearly extra-logical (it does not commute with instantiation). However, the high-level programming construct \verb+delay+ \ldots \verb+until not flexible+ that we have just encoded is perfectly logical.

We also have an experimental, CHR-inspired syntax to propagate constraints, but we omit its explanation from the paper because it is still work in progress. We only note here that propagation is a logical operation iff the propagation rule can be proved to holds as a meta-theoretical theorem. Therefore, so far, the proposed extensions are fully in the spirit of logic programming.

\section{The kernel in LamdbaProlog}

\section{Parsing and Pretty Printing}

\section{Tactics and Tacticals}

\section{Definitional Mechanisms}

\section{Conclusions and Future Work}

\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}


\end{document}
