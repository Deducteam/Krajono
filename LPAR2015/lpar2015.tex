% LLNCStmpl.tex
% Template file to use for LLNCS papers prepared in LaTeX
%websites for more information: http://www.springer.com
%http://www.springer.com/lncs

\documentclass{llncs}
%Use this line instead if you want to use running heads (i.e. headers on each page):
%\documentclass[runningheads]{llncs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{color}
\usepackage{float}

\floatstyle{plain} \newfloat{example}{thp}{lop} \floatname{example}{\textbf{Example}}

\begin{document}
\title{ELPI: A Fast, embeddable, \lp{} interpreter}

%If you're using runningheads you can add an abreviated title for the running head on odd pages using the following
%\titlerunning{abreviated title goes here}
%and an alternative title for the table of contents:
%\toctitle{table of contents title}

%\subtitle{Subtitle Goes Here}

%For a single author
%\author{Author Name}

%For multiple authors:

\author{Cvetan Dunchev$^1$ \and
        Ferruccio Guidi$^1$ \and
        Claudio Sacerdoti Coen$^1$ \and
        Enrico Tassi$^2$}


%If using runnningheads you can abbreviate the author name on even pages:
%\authorrunning{abbreviated author name}
%and you can change the author name in the table of contents
%\tocauthor{enhanced author name}

%For a single institute
\institute{Department of Computer Science,
University of Bologna,~%\\ Mura Anteo Zamboni 7, 40127 Bologna, Italy \\
\email{name.surname@unibo.it}
\and Inria Sophia-Antipolis,~%\\ 2004 route des Lucioles - BP 93, 06902 Sophia Antipolis Cedex, France
\email{name.surname@inria.fr}}

% If authors are from different institutes
%\institute{First Institute Name \email{email address} \and Second Institute Name\thanks{Thank you to...} \email{email address}}

%to remove your email just remove '\email{email address}'
% you can also remove the thanks footnote by removing '\thanks{Thank you to...}'

\newcommand{\frag}{Reduction-Free Fragment}
\newcommand{\lp}{$\lambda$Prolog}
\newcommand{\Ll}{\ensuremath{\mathcal{L}_\lambda}}
\newcommand{\elpi}{ELPI}
\newcommand{\tedius}{Teyjus}
\newcommand{\CSC}[1]{\textcolor{red}{#1}}
\newcommand{\FG}[1]{\textcolor{magenta}{#1}}

\maketitle

\begin{abstract}
We present a new interpreter for \lp{} that runs consistently faster than
the byte code compiled by \tedius{}, that is believed to be the best
available implementation for \lp. 
The key insight is the identification of a fragment of
the language, which we call \frag{}, that occurs quite naturally in \lp{}
programs and that admits constant time reduction and unification rules.
% The implementation exploits De Bruijn levels and no explicit substitutions,
% whereas \tedius{} is based on De Bruijn indexes and explicit substitutions
% (the suspension calculus).
\end{abstract}

\section{Introduction}% and State of the Art}
\lp{} is a logic programming language based on an intuitionistic fragment of
Church's Simple Theory of Types. An extensive introduction to the language
with examples can be found in~\cite{Miller:2012:PHL:2331097}. \tedius{}
\cite{DBLP:conf/cade/NadathurM99,DBLP:journals/corr/abs-0911-5203} is a
compiler for \lp{} %implemented by Gopalan Nadathur and others
that is considered to be the fastest
implementation of the language. 
%Previous slower implementations are described
%in~\cite{}. \CSC{OTHER IMPLEMENTATIONS IN ISABELLE ETC. MISSING}
The main difference with respect to Prolog is that \lp{} manipulates
$\lambda$-tree expressions, i.e. syntax containing binders. Therefore the
natural application of \lp{} is meta-programming (see~\cite{LPAZ} for
an interesting discussion), including: automatic generation of programs from
specifications; animation of operational semantics;
program transformations and implementation of type checking algorithms.

Via the Curry-Howard isomorphism, a type-checker is a proof-checker, the main
component of an interactive theorem prover (ITP). Indeed the motivation of our
interest in \lp{} is that we are looking for the best 
language to implement the so called \emph{elaborator} component of an ITP.
The elaborator is used to type check the terms input by
the user.  Such data, for conciseness reasons, is typically incomplete and
the ITP is expected to infer what is lacking.  The possibility to
extend the built in elaborator with user provided ``logic programs'' (in the
form of type classes or unification hints) to infer the missing pieces of
information turned out to be a key ingredient in successful formalizations
like~\cite{gonthier:hal-00816699}.  Embedding in an ITP a \lp{} interpreter
would enable the elaborator and its extensions to be expressed in the same,
high level, language.  A crucial constraint is the efficiency of the embedded
\lp{} interpreter.

In this paper we introduce ELPI, a fast \lp{} interpreter that being written
completely in OCaml can be easily embedded in OCaml softwares, like Coq.
In particular we focus on the insight that makes ELPI fast when dealing with
binders by identifying a reduction free fragment (RFF) of \lp{} that, if
implemented correctly, has constant-time unification and reduction operations.
We analyze the role  of beta reduction in section 2, ho unification in section 3, define the fragment in section 4, asses the result in section 5.

% We don't mention constraints here, too much vapor ware IMO.
%\lp{} with Constraints (\`a la CLP) is the best choice.

\section{The two roles of $\beta$-reduction in \lp{}}

\begin{example}[b]
\begin{center}
\begin{tabular}{c@{~~}|@{~~}c}
\begin{minipage}{4.8cm}
\begin{verbatim}
of (app M N) B :-
  of M (impl A B), of N A.
of (lam F) (impl A B) :-
  pi x\ of x A => of (F x) B.
\end{verbatim}
\end{minipage}
&
\begin{minipage}{6.2cm}
\begin{verbatim}
cbn (lam F) (lam F).
cbn (app (lam F) N) M :- cbn (F N) M.
cbn (app M N) R :-
  cbn M (lam F), cbn (app (lam F) N) R.
\end{verbatim}
\end{minipage}
\end{tabular}
\end{center}
\caption{\label{example1} Type checker and Weak CBN for simply typed $\lambda$-calculus.}
\end{example}

We introduce \lp{} and discuss the role of $\beta$-reduction on the
Example~\ref{example1}. The $\lambda$-term %$\Delta$ defined as 
$(\lambda x.xx)$ is encoded %in $\lambda$-tree syntax 
as \verb+(lam (x\ app x x))+ where \verb+x\F+ is
the $\lambda$-abstraction of \lp{}, that binds \verb+x+ in \verb+F+, and
\verb+lam+ is the constructor for object-level abstraction, that builds
a term of type $\mathcal{T}$ from a function of type
$\mathcal{T} \to \mathcal{T}$, with $\mathcal{T}$ the type of representations
of $\lambda$-terms. \verb+app+ takes two terms of type $\mathcal{T}$ and builds
their object-level application of type $\mathcal{T}$.  Following the tradition of Prolog, capitals letters denote unification variables.

The second clause for the \verb+of+ predicate shows a recurrent pattern in
\lp: in order to analyze an higher order term, one needs to do the recurse
under a binder. This is achieved
exploiting the forall quantifier \verb+pi x\G+ together with logical
implication \verb+F(x) => G+. Operationally: the forall quantifier declares a
new local constant \verb+x+, meant to be fresh in the entire program;
logical implication temporarily
augments the proof context with the new formula \verb+F+ about \verb+x+.
Denotationally, these are just the standard rules for introduction of
implication and the universal quantifier.

Note that the functional (sub-)term
\verb+F+ is applied to the fresh constant \verb+x+.  Being \verb+F+ a
function, the $\beta$-redex \verb+(F x)+, once reduced, denotes the body of
our object-level function where the bound variable is replaced by the fresh
constant \verb+x+.
The implication is used to
assume \verb+A+ to be the type of \verb+x+, in order to prove that the body of
the abstraction has type \verb+B+ and therefore the whole abstraction has type
\verb+(impl A B)+ (i.e. $A \to B$). Note that, unlike in
the standard presentation of the typing rules, we do not need to manipulate an
explicit context $\Gamma$ to type the free variables. Instead the assumptions
of the form \verb+of x A+ are just added to the program clauses, and \lp{}
takes care of dropping them from the context when \verb+x+ goes out of scope.

If the initial goal is
\verb+of (lam (w\ app w w)) T+ by applying the second clause we assign
\verb+(impl A B)+ to \verb+T+ and generate
a new goal \verb+of (app c c) B+ (where \verb+c+ is the fresh constant
substituted for \verb+w+) to be solved with the extra clause \verb+of c A+
at disposal.

In this first example, the meta-level $\beta$-reduction is only employed
to inspect a term under a binder by replacing the bound name with a fresh
constant.  The second example
shows a radically different pattern: in order to implement object-level
substitution --- and thus object-level $\beta$-reduction --- we use the
meta-level $\beta$-reduction. E.g. if \verb+F+ is \verb+(w\ app w w)+
then \verb+(F N)+ reduces to \verb+(app N N)+.  Note that in this case
$\beta$-reduction is fully general, because it replaces a name with a term.
This distinction is crucial in the definition of the RFF in
Section~\ref{sec:RFF}.

\section{Higher Order unification}% and captures}

Unification in the presence of binders raises two both theoretical and
practical concerns. First, the absence of most general unifiers (MGUs) makes
one of the primitive operations of \lp{} very delicate.  Second, avoiding
captures (i.e. checking a unification variable is only instantiated with a
term containing bound variable in scope) is both tricky and, as we will see,
expensive.

To cope with the absence of MGUs, Dale Miller identified
in~\cite{Miller91alogic} a well-behaved fragment (\Ll{}) of higher-order
unification that admits MGUs and is stable under \lp{} resolution. 
The restriction defining \Ll{} is that unification variables can
only be applied to (distinct) variables (i.e. not arbitrary terms) that are
not already in the scope of the variable.
Such fragment can effectively serve as a primitive for a
programming language and indeed \tedius{} 2.0 is built around this fragment:
no attempt to enumerate all possible unifiers is performed, and unification
%\marginpar{cite huet? no space IMO}
problem falling outside \Ll{} are just delayed.  Many interesting \lp{}
programs can be rewritten to fall in the fragment.
For example the \verb+cbn+ one as written in example 1 falls outside \Ll{},
since it contains \verb+(F N)+, but can be coded expressing directly
the substitution of the meta-language in \lp{} as follows.
\begin{center}
\small
\begin{minipage}{10cm}
\begin{verbatim}
cbn (app (lam F) N) M :- subst F N B, cbn B M.
subst F N B :- pi x\ copy x N => copy (F x) B.
copy (lam F1) (lam F2) :- pi x\ copy x x => copy (F1 x) (F2 x).
copy (app M1 N1) (app M2 N2) :- copy M1 M2, copy N1 N2.
\end{verbatim}
\end{minipage}
\end{center}
The idea of \verb+subst+ is that the term \verb+F+ is recursively copied in
the following way: each bound variable is copied in itself but for the top one
that is replaced by \verb+N+.
The interested reader can find an longer discussion about \verb+copy+
in~\cite[page 199]{Miller:2012:PHL:2331097}.  
The \verb+of+ program falls naturally in \Ll{}, since \verb+F+ is only applied
the fresh variable \verb+x+ (all unification variables in a \lp{} program are
implicitly existentially bound in front of the clause, so \verb+F+ does not
see \verb+x+ already).  The same holds for \verb+copy+.

To correctly implement HO unification, even in the restricted \Ll{} fragment,
one typically tracks the \emph{level} of unification variables an fresh constants.  The proof theoretic interpretation of a \lp{} execution as an intuitionistic proof gives the following reading: unification is taking place under
a mixed prefix of $\forall$ and $\exists$ quantifiers; their ordered
determines if a unification variable (an existential) can be assigned to
(proved by) a term that contain a universally quantified variable.
E.g. $\forall x,\exists Y, Y = x$ is always provable while
$\exists Y,\forall x, Y = x$ is not.
\marginpar{rephrase with symbols}
Whenever a clause is used, its unification variables are declared at
a level that corresponding the length of the current ``context'', and whenever
a fresh constant is created (for the \verb+pi+ quantifier) the context
is extended and the constant is placed at such level.  From now on we will
write levels in superscript.  If we run the program
\marginpar{sketched}
\verb+(of (lam f\lam w\app f w) T)+ after two steps the goal is
\verb+(of (app c+$^1$\verb+ d+$^2$\verb+) T+$^0$\verb+)+. T is at level 0 because we don't pass to it x in the lam rule, so we never extend its range.
Replacing f and w by fresh constant anntoated with a level is the implemetation
technique adopted by \tedius{} to make unif cation be able to check
levels correctly.  to avoid traversing the term at each pi it
performs the beta reduction in a lazy way using an explicit substitution calculus.  In the RFF we will be able to completely avoid such substitution.


% 
% \begin{table}
% \begin{center}
% \begin{tabular}{c@{~~}|@{~~}c}
% \begin{minipage}{5.0cm}
% \begin{verbatim}
% forall P [].
% forall P [X|XS] :-
%    P X, forall P XS.
% \end{verbatim}
% \end{minipage}
% &
% \begin{minipage}{5.0cm}
% \begin{verbatim}
% (pi Y\  Y :- pi x0\ r Y) =>
%    (pi x0\ q (x1\ X^0 x0 x1))
% \end{verbatim}
% \end{minipage}
% \end{tabular}
% \end{center}
% \caption{\label{example4} Example 4 (on the left): an higher order predicate whose definition is not in the pattern fragment. Example 5 (on the right): all clauses are in the \frag, but after two inferences the obtain term is not.}
% \end{table}
% 
% The pattern fragment, discovered by Dale Miller in~\cite{???}, is a
% well-behaved fragment of higher-order unification that is stable under \lp{}
% resolution. I.e. if the syntax of the program is in the fragment, all
% unification problems will only involve terms that are still in the fragment.
% Moreover, all unification problems in the pattern fragment admit a most
% general unifier, even in the flexible-flexible and flexible-rigid case. The
% fragment is easily defined as follows: a variable \verb+X+ can only be
% applied to a list of distinct names, and only if those names are not in the
% scope of \verb+X+.
% Example 1 is written in the pattern fragment: \verb+F+ is applied to \verb+x+,
% and \verb+F+ is universally quantified before \verb+x+, that therefore is not
% in the scope of \verb+F+. Example 2, instead, is not in the fragment because
% of \verb+F N+ where \verb+N+ is not a name. However, at the expense of some
% verbosity, it is possible to rewrite Example 2 to force it in the pattern
% fragment (see Table~\ref{example3}). The definition of higher order predicates
% like in Table~\ref{example4}, is, however, typically outside the pattern
% fragment.

\section{Bound variables}

The last missing ingredient to define the RFF and explain why it can be
implemented efficiently is to see how systems manipulating $\lambda$-terms
accommodate
$\alpha$-equivalence.  Bound variables are not represented by using
real names, but canonical ``names'' (or better numbers).  De Bruijn introduced
two, dual, canonical naming schemas for lambda terms: indexes (DBI) and levels
(DBL).  In the former a variable is named $n$ if its binder is found by
crossing $n$ binders going in the direction of the root.  In the latter a
variable named $n$ is bound by the $n$-th binder one encounters in the path
from the root to the variable.

\begin{table}
\begin{center}
\begin{tabular}{c@{~~}|@{~~}c}
	Indexes & Levels\\\hline
$\lambda x.(\lambda y.\lambda z.f~x_2~y_1~z_0)~x_0$ &
$\lambda x.(\lambda y.\lambda z.f~x_0~y_1~z_2)~x_0$ \\
$\lambda x.\lambda z.f~x_1~x_1~z_0$ &
$\lambda x.\lambda z.f~x_0~x_0~z_1$ \\
\end{tabular}
\end{center}
\caption{\label{example2}
De Bruijn notations for
$\lambda x.(\lambda y.\lambda z.f~x~y~z)~x$
and its reduct}
\end{table}

In both notations when a binder is removed and the corresponding variable
substituted some ``renaming'' (called lifting) has to be performed.  
The DBI convention is way more popular than the DBL one and \tedius{}
indeed adopts that schema.  We believe the popularity of DBI comes from the
fact that weak head normalization is easier to code: the argument of the
redex, being at the top level of the term, is always closed and hence
invariant by lifting.

In \elpi{} we chose DBL by observing the following two phenomena:
first, the name of a variable is context independent, i.e. $x$ is
always named $x_0$, while with indexes its name depends on the depth at
which it occurs; second
when $\beta$-reduction occurs deep in the term, here under the context
of the $x$ variable, the occurrences of the variables in the context do
not change name, i.e. $x$ is named $x_0$ in the initial and in the reduct
term.  Another way to put it is that variables already pushed in the context
are treated exactly as constants, and their name is exactly the level at
which the occur in the context.  From now on, we subscript bound variables
with their corresponding name in DBL convention.

I HAVE ENOUGH FOR TODAY


%One important property of the pattern fragment is that $\beta$-reduction can
%only substitute names with other names. This property, however, is not
%easily exploitable and an implementation still needs to traverse the body
%of the abstraction to do the replacing. If we look at Example 1, we see that
%in order to type-check a $\lambda$-abstraction the operational semantics
%requires to first traverse all the body just to replace a bound name with a
%fresh one. This is a major source of inefficiency that is hardly justified.
%To cope with it, \tedius{} 2.0 invented the \emph{suspension calculus}~\cite{susp1,susp2}, a form of explicit substitution calculus that propagates the substitution lazily (and that works also outside the pattern fragment). Some benchmarks~\cite{susp3} concluded that adopting the suspension calculus provides a significant speedup over a naive implementation of susbtitution. Nevertheless, programs compiled with \tedius{} are quite slow and, at least in cases like Example~1, the whole idea of substituting bound names with fresh names, in the spirit of the locally nameless approach~\cite{???}, seems a waste of time.
%
%In~\ref{sec:fragment} we will identify a sub-fragment of the pattern fragment
%that we call~\frag. A clever implementation can completely avoid the traversal of the body of the abstraction during $\beta$-reduction, if the term to be reduced is in the~\frag.
%In the remaining sections we will present an interpreter
%for \lp, written in OCaml and called \elpi{} (Embedded \lp{} Interpreter), that exploits the \frag, and we will compare it to \tedius{} on a few benchmarks.
%We anticipate that \elpi{} is consistently faster than \tedius{} on every test, sometimes up to \CSC{XXXX} times.

\section{The~\frag.}\label{sec:fragment}
The abstract syntax of \lp{} is defined as follow:
$$\begin{array}{l}
   Q ::= \forall x.Q ~|~ P \Rightarrow Q ~|~ Q \wedge Q ~|~ T ~|~ \exists X.Q \\
   P ::= \forall X.P ~|~ Q \Rightarrow P ~|~ P \wedge P ~|~ T\\
   T ::= c ~|~ x ~|~ X ~|~ \lambda x.A ~|~ T~A\\
   A ::= Q ~|~ P
\end{array}$$
where $c,x,X$ range respectively over the set of constants, the set of universally quantified names (also called local constants) and the set of existentially quantified variables. Formulas $Q$ that occur in positive positions are called \emph{queries}. Formulas $P$ that occur in negative position are called \emph{clauses}. The logical atoms are captured by $T$. Note that, the language being
higher order, there is no distinction between formulas and terms, and both queries and clauses can be passed as arguments to predicates. Note also that $\forall$ binds local constants in positive position, and existentially quantified variables in negative position, according to the logical equivalence
$(\forall x.P) \Rightarrow Q \equiv \exists x.(P \Rightarrow Q)$. Disjunction
is definable, but the derivational semantics is complete only if it is used in
positive formulas (like for the existential quantifiers). We let $\mathcal{E}$ range over all syntactic categories above, and we call $\mathcal{E}$ an expression.

The concrete syntax differs from the abstract syntax in a few ways: \verb+pi x\+ and \verb+sigma x\+ are used in place of $\forall x.$ and $\exists x.$; $P \Rightarrow Q$ can be written as \verb+P => Q+ or as \verb+Q :- P+; conjunction is written using commas; $\lambda x.$ is written \verb+x\+. A program is just a list of clauses terminated by dots. All free uppercase names in a clause are implicitly universally quantified around the clause. The user writes a query to be resolved against a program. All free uppercase names in the query are also implicitly existentially quantified. The semantics of the pair program/clause is a new query obtained as the implication between the $n$-ary conjunction of all the clauses and the user provided query.

$\lambda x.A$ and $\forall x.Q$ bind $x$ respectively in $A$ and $Q$;
$\forall X.P$ and $\exists X.Q$ bind $X$ respectively in $P$ and $Q$.
Formulas are identified up to $\alpha$-equality as usual.

\paragraph{Definition and expressivity of the \frag}
The De Bruijn level of a name $x$ in an expression is the number of binders to be crossed when traversing the expression from the root towards the leaves before finding the binder for $x$~\cite{debruijnlevel}. For example, in the following expression every variable $x_i$ has level $i$: $\forall x_0.\exists X^1.\lambda x_1.\lambda x_2. \exists X^3. p~(X^1~x^1) X^3$. Every expression can be
$\alpha$-converted so that every name is renamed to $x_i$ where $i$ is its level. In the rest of the discussion we assume every term to be renamed in this way. Similarly, we can rename every variable $X$ to $X^j$ where $j$ is the smallest number such that $X^j$ has visibility of all names smaller than $j$. In the example above, $X^1$ has visibility only of $x^0$, and $X^3$ has visibility of $\{x^0,x^1,x^2\}$.

An expression is in the \frag{} iff every occurrence of an existentially quantified $X^j$ occurs applied to $x_j \ldots x_{j+k-1}$ for $k >= 0$.

Examples: $X^2~x_2~x_3$ and $X^2$ are in the fragment; $X^2~x_3$ and $X^2~x_3~x_2$ are not.

Observe that Example 1 and 3 in Tables~\ref{example1,example3} are in \frag, and that most occurrences of existentially quantified variables in Example 2 are in \frag{} as well. Every Prolog program is also in \frag. As we will see in~Section~\ref{grundlagen}, a type-checker for a dependently typed language and and evaluator based on a reduction machine are also naturally in \frag, showing that the fragment is already quite expressive.

\paragraph{Computational properties of the \frag}
First of all, observe that the \frag{} is a sub-fragment of the pattern fragment. Therefore an unification problem $X^j~x_j\ldots x_{j+k-1} \equiv \mathcal{E}$ admits a most general unifier.

The most interesting property of the \frag, which also justify its name, is the following. Let $\sigma$ be a \emph{valid} substitution for existentially quantified variables, i.e. such that for every $X^j$ all names $x_i$ occurring in $X^j \sigma$ are s.t. $i < j$. Let $X^j \sigma = \lambda x_j. \ldots \lambda x_{j+n}.\mathcal{E}$. Then
\begin{equation}\label{deffrag}(X^j~x_j \ldots x_{j+k-1}) \sigma
 = \left\{ \begin{array}{ll}
\mathcal{E}~x_{j+n+1} \ldots x_{j+k-1} & \mbox{if $n+1 < k$} \\
\lambda x_{j+k}. \ldots \lambda x_{j+n}.\mathcal{E} & \mbox{otherwise}
      \end{array} \right.\end{equation}

In equation~\ref{deffrag}, the equality is syntactical, i.e. no $\alpha$-conversion of $\mathcal{E}$ is required. Therefore the $\beta$-reductions triggered
by the susbtitution of $X^j$ require constant time and no pass over the term.
Note that, once all $\beta$-redex that occur syntactically in the term are
fired at compile time, or if explicitly written $\beta$-redexes are forbidden,
then every $\beta$-redex in the computation is triggered by the substitution
of an existentially quantified variable. Therefore, if the program is and
remains in the \frag, then all $\beta$-reductions can be implemented in
constant time.

In a similar way, also unification can be implemented very easily observing
that $X^j~x_j\ldots x_{j+k-1}$ can always be rewritten as
$X^{j+k}$ instantiating $X^j$ with $\lambda x_j. \ldots \lambda x_{j+k-1}. X^{j+k}$ for a fresh existentially quantified variable $X^{j+k}$. Therefore every
unification problem in the \frag{} can be reduced in linear time in $k$ to a
problem of the kind $X^{j+k} \equiv \mathcal{E}$, that is immediately solved
by instantiating $X^{j+k}$ with $\mathcal{E}$ if the free names of $E$ are
known to be a subset of $\{x_0,\ldots,x_{j+k-1}\}$.

Finally, another benefit of working with De Bruijn levels is that the
$\forall$-introduction rule can also be implemented in constant time as well
because there is no need to replace the bound variable with a fresh name.
The price to pay is an additional complexity in the backchain rule: when
a clause is selected, the clause needs to be $\alpha$-converted --- lifted
in De Bruijn indexes/levels terminology --- to move it in the current scope.
For example, if the query is \verb+ pi x_0\ r x_0+ and \verb+r X :- pi x_0\ pi x_1\ p x_0 x_1 X+ is a clause, after two inferences the new query becomes
\verb+pi x_1\ pi x_2\ p x_1 x_2 x_0+ because the introduction rule for
\verb+pi+ has already fixed the name \verb+x_0+.

\paragraph{Instability of the \frag}
Unlike the pattern fragment, the \frag{} is unstable for computation.
I.e. there are simple examples where the program and the initial query are
written in the fragment, but during execution we generate terms outside the
fragment (but still in the pattern fragment). This is a consequence of the
need for lifting clauses during backchaining.
Consider example~5 in Table~\ref{example4}.
After two inference steps the term \verb+(x1\ X^0 x0 x1)+, that is in
the fragment and assigned to \verb+Y+, is moved under \verb+pi x0+ becoming
\verb+r (x2\ X^0 x0 x2)+. The latter is no longer in the fragment.

Therefore, and contrarily to what happens with the pattern fragment, an
implementation must handle both terms in the fragment, with their efficient
computation rules, and terms outside the fragment. Our limited experience so
far, however, is that several programs initially written in the fragment
remains in the fragment during computation, or they can be slightly modified
to achieve that property.

\section{\elpi: an Embedded \lp{} Interpreter.}\label{sec:elpi}
The current version of \elpi, together with the benchmarks presented in
the next section, can be downloaded at \CSC{\url{http://xxxxx}}. The intepreter
is entirely written in OCaml and it is \CSC{XXX} lines long. It augments the language presented in the paper with Prolog-style cuts and a few custom predicates for printing. The name is due to the fact that we eventually plan to augment the language with constraints and then embed it in the implementation of interactive theorem provers like Coq, that are often written in OCaml.

We carefully wrote the code so that unrechable terms in \lp{} are encoded by unreachable terms in OCaml. Therefore we inherit garbage collection from the OCaml runtime. The algebraic datatype used to encode terms has different constructors for occurrences of terms in the \frag, and for occurrences outside the fragment. In particular an occurrence $X^j x_j \ldots x_{j+k-1}$ is simply represented as a triple $\langle r,j,k\rangle$ where $r$ is a reference to the term
$X$ is instantiated to, or to a dummy term if $X$ is still unbound. Finally, we carefully cherry picked some design decisions from the WAM, implementing our own variations in several places. The variations have been made necessary to decrease the pressure on the garbage collector of OCaml, that is responsible for a significant percentage of the running time. For example, the heap and the stack are not organized as large arrays of mostly unused cells because otherwise the garbage collector needs to traverse the arrays at every minor and major collection. Similarly, we employ a mix of mutable and persistent data structures, avoiding mutable structures for data that is likely to survive, in order to avoid the penalty of the write barrier on data in the old generation~\cite{???}.
Further details will be reported in a future work due to lack of space.

\section{Benchmarks.}\label{sec:benchmarks}

\input{helena}

In the following table we compare the performance of ELPI and Teyjus over
several benchmarks divided into 3 groups.  The first group contains 4
first-order problems taken from the Aquarius suite. The problems are the well
known logic games such as the crypto-multiplication, $\mu$-puzzle,
generalized eight queens problem and the Einstein's zebra puzzle,
respectively.

The second group contains 2 higher-order problems within the fragment. The
first one builds the $n$-th projection and type checks it with the \verb+of+
program in Example 1 a thousands of times.
Due to limitations in \tedius{} we reached a max
value of 321 for $n$.
% fails to manipulate terms with nesting factor more than $2^{16}$. 
The second
example shows a few applications of the {\bf cbv} function defined above over
the Church numerals, the larger one being computed is $5^5$.

The third group contains a single higher-order example from the \tedius{}
suite.  It is outside the fragment. 

\begin{center}

  \begin{tabular}{|p{1.5cm}||c|r||c|r||c|c|}
    \hline
      \multicolumn{1}{|c||}{Test} &
      \multicolumn{2}{|c||}{ELPI} &
      \multicolumn{2}{|c||}{\tedius{}} &
      \multicolumn{2}{|c|}{Ratio(ELPI/Teyjus)} \\
    \hline
      &  time     & memory  & time & memory &  time & memory \\
    
      &  \multicolumn{1}{c|}{(s)}  & \multicolumn{1}{c||}{(Kb)} & \multicolumn{1}{c|}{(s)}  & \multicolumn{1}{c||}{(Kb)} & \multicolumn{1}{c|}{(s)}  & \multicolumn{1}{c|}{(Kb)} \\
    \hline
    \hline
    crypt &  3.46 & 25,828  & 6.59 & 18,048 &  0.52 & 1.43 \\
    \hline    
    mu &  1.83 & 5,716 &  3.62 & 50,076 &  0.50 & 0.11 \\
    \hline
    queens &  1.33  & 108,140 &  2.02 & 69,968 &  0.65 & 1.54 \\
    \hline    
    zebra &  0.85 & 6,944 &  1.89 & 8,412 &  0.44 & 0.82 \\
    \hline     
    \hline
    lambda3 &  0.30 & 6,080 &  5.64 & 239,892 &  0.05 & 0.02 \\
    \hline
    reduce &  0.16 & 8,500 &   9.04 & 74,624  & 0.02 & 0.11 \\
    \hline
    \hline
    ski &  1.30 & 21,668 &  2.68 & 8,896  & 0.48 & 2.43 \\
    \hline
    
  \end{tabular}

 \end{center}



 
 
\bibliographystyle{plain}
\bibliography{reference}

\end{document} 
